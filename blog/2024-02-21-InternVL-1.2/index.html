<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <!-- Begin Jekyll SEO tag v2.8.0 -->
        <title>InternVL 1.2: Scaling up LLM to 34B</title>
        <meta name="generator" content="Jekyll v3.9.4"/>
        <meta property="og:title" content="InternVL 1.2: Scaling up LLM to 34B"/>
        <meta name="author" content="chenzhe"/>
        <meta property="og:locale" content="en_US"/>
        <meta property="og:site_name" content="InternVL"/>
        <meta property="og:type" content="article"/>
        <meta property="article:published_time" content="2024-01-30T12:33:38-06:00"/>
        <meta name="twitter:card" content="summary"/>
        <meta property="twitter:title" content="InternVL 1.2: Scaling up LLM to 34B"/>
        <!-- End Jekyll SEO tag -->
        <link rel="icon" href="https://github-production-user-asset-6210df.s3.amazonaws.com/47669167/330728723-7037290e-f474-4d11-b90f-1d8316087bf8.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240529%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240529T072300Z&X-Amz-Expires=300&X-Amz-Signature=d12b9e5c3c49a082747f5da55529a4f1247cd17b4329fafc1cb6d1c0678efa77&X-Amz-SignedHeaders=host&actor_id=23737120&key_id=0&repo_id=721995615">
        <link rel="stylesheet" href="/blog/assets/main.css">
        <link type="application/atom+xml" rel="alternate" href="https://llava-vl.github.io/blog/feed.xml" title="LLaVA"/>
    </head>
    <body>
        <header class="site-header" role="banner">
            <div class="wrapper">
                <a class="site-title" rel="author" href="/blog/">InternVL</a>
                <nav class="site-nav">
                    <input type="checkbox" id="nav-trigger" class="nav-trigger"/>
                    <label for="nav-trigger">
                        <span class="menu-icon">
                            <svg viewBox="0 0 18 15" width="18px" height="15px">
                                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
                            </svg>
                        </span>
                    </label>
                    <div class="trigger"></div>
                </nav>
            </div>
        </header>
        <main class="page-content" aria-label="Content">
            <style>
                @media (max-width: 768px) {
                        img.responsive {
                        width: 100% !important;
                    }
                }
            </style>
            <div class="wrapper">
                <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
                    <header class="post-header">
                        <h1 class="post-title p-name" itemprop="name headline">InternVL 1.2: Scaling up LLM to 34B</h1>
                        <p class="post-meta">
                            <time class="dt-published" datetime="2024-01-30T12:33:38-06:00" itemprop="datePublished">2024/02/12
      </time>
                            ‚Ä¢
                            <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                                <span class="p-author h-card" itemprop="name">Zhe Chen, Weiyun Wang, Wenhai Wang, Erfei Cui, Zhangwei Gao, Xizhou Zhu, Lewei Lu, Tong Lu, Yu Qiao, Jifeng Dai
                                </span>
                            </span>
                        </p>
                    </header>

                    <p><a rel="nofollow" href="../">[üÜï Go Back]</a>  <a rel="nofollow" href="https://arxiv.org/abs/2312.14238">[üìú InternVL 1.0 Paper]</a>  <a rel="nofollow" href="https://arxiv.org/abs/2404.16821">[üìú InternVL 1.5 Report]</a>  <a rel="nofollow" href="https://internvl.opengvlab.com/">[üó®Ô∏è Chat Demo]</a>  <a rel="nofollow" href="https://huggingface.co/spaces/OpenGVLab/InternVL">[ü§ó HF Demo]</a>  <a rel="nofollow" href="https://modelscope.cn/models/OpenGVLab/Mini-InternVL-Chat-4B-V1-5/summary">[<img src="images/modelscope_logo.png" width="20px" style="max-width: 100%;"> ModelScope]</a>  <a rel="nofollow" href="https://github.com/OpenGVLab/InternVL?tab=readme-ov-file#quick-start-with-huggingface">[üöÄ Quick Start]</a> <a rel="nofollow" href="https://zhuanlan.zhihu.com/p/702946079">[üìñ ‰∏≠ÊñáËß£ËØª]</a></p>


                    <div class="post-content e-content" itemprop="articleBody">
                        <!-- ÊèíÂÖ•MarkdownËΩ¨Êç¢ÂêéÁöÑHTMLÂÜÖÂÆπ -->

<table>
                        <thead>
                        <tr>
                        <th>Type</th>
                        <th>Model</th>
                        <th>Date</th>
                        <th>Download</th>
                        <th>Note</th>
                        </tr>
                        </thead>
                        <tbody>

                        <tr>
                        <td rowspan="2">Vision Large Language Model</td>
                        <td>InternVL-Chat-V1-2-Plus</td>
                        <td>2024.02.21</td>
                        <td>ü§ó <a href="https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2-Plus" rel="nofollow">HF link</a></td>
                        <td>more SFT data and stronger</td>
                        </tr>

                        <tr>
                        <td>InternVL-Chat-V1-2</td>
                        <td>2024.02.11</td>
                        <td>ü§ó <a href="https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2" rel="nofollow">HF link</a></td>
                        <td>scaling up LLM to 34B</td>
                        </tr>

                        <tr>
                         <td>Vision Foundation Model</td>
                            <td>InternViT-6B-448px-V1-2</td>
                        <td>2024.01.30</td>
                        <td>ü§ó <a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-2" rel="nofollow">HF link</a></td>
                        <td>vision foundation model, 448 resolution</td>
                        </tr>

                        </tbody>
                    </table>

                        <h2>InternVL-Chat-V1-2</h2>


                        <p>We are excited to introduce InternVL-Chat-V1-2. Inspired by <a href="https://llava-vl.github.io/blog/2024-01-30-llava-next/">LLaVA-NeXT-34B</a>, we have also adopted <a href="https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B">Nous-Hermes-2-Yi-34B</a> as the language model. Below is the pipeline.</p>

                        <p><img class="responsive" width="650" alt="image" src="images/Intern1_2_1.png"></p>

                        <p>From the experimental results, <strong>we've observed that a stronger language model (34B) can better leverage the powerful capabilities of our vision foundation model.</strong></p>

                        <p>For better training reproducibility, we follow the minimalist design and data efficiency similar to LLaVA-NeXT. To reduce training costs, we provide a <a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-2/blob/main/mlp_projector/hermes_2_yi_34b.pth">pre-trained MLP projector</a> and only employ around 1.2 million visual instruction tuning samples for SFT. Our model has a total of 40 billion parameters and can be trained within 1.5 days using 32 A100 GPUs. The code, data, and model have been made publicly available.</p>

                        <h3>Data Preparation</h3>

                        <p>Inspired by LLaVA-NeXT, we adopted a data-efficient SFT strategy to train InternVL-Chat-V1-2, utilizing approximately 1.2M of visual instruction tuning samples in total, all of which are fully open-source. In a macro sense, we build upon <a href="https://github.com/InternLM/InternLM-XComposer/blob/main/projects/ShareGPT4V/docs/Data.md#prepare-images">ShareGPT-4V</a> and additionally integrate <a href="https://huggingface.co/datasets/openbmb/llava_zh">LLaVA-ZH</a>, <a href="https://github.com/kushalkafle/DVQA_dataset">DVQA</a>, <a href="https://github.com/vis-nlp/ChartQA">ChartQA</a>, <a href="https://allenai.org/data/diagrams">AI2D</a>, <a href="https://www.docvqa.org/datasets">DocVQA</a>, <a href="https://github.com/SCNU203/GeoQA-Plus">GeoQA+</a>, and <a href="https://huggingface.co/datasets/naver-clova-ix/synthdog-en">SynthDoG-EN</a>. Most of the data remains consistent with LLaVA-NeXT.</p>

                        <h3 id="model-card">Model Card</h3>

                        <table style="text-align: center;">
                          <tr><th colspan="2">Name</th><th>InternVL-Chat-V1-2</th><th>InternVL-Chat-V1-2-Plus</th></tr>
                          <tr><th rowspan="4">Model Size</th><td>Total</td><td colspan="2"><b>40.07B</b></td></tr>
                          <tr><td>ViT</td><td colspan="2">5.54B</td></tr>
                          <tr><td>MLP</td><td colspan="2">143.17M</td></tr>
                          <tr><td>LLM</td><td colspan="2">34.39B</td></tr>
                          <tr><th colspan="2">Resolution</th><td colspan="2">448 √ó 448</td></tr>
                            <tr><th rowspan="2"><nobr>Stage-1</nobr></th><th>Training Data</th><td colspan="2">Trained on 39.3 million samples, including COYO, LAION, CC12M, CC3M, SBU, Wukong, GRIT, Objects365, OpenImages, and OCR-related datasets. In this stage, we first load the pre-trained weights of <a rel="nofollow" href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-0">InternViT-6B-448px-V1-0</a> and connect it to Nous-Hermes-2-Yi-34B. After pre-training, the extracted ViT is published as <a rel="nofollow" href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-2">InternViT-6B-448px-V1-2</a>.</td></tr>
                          <tr><th>Trainable Module</th><td colspan="2">ViT + MLP</td></tr>
                            <tr><th rowspan="2">Stage-2</th><th>Training Data</th><td>Open-sourced 1.2M visual instruction tuning samples. </td><td>A comprehensive collection of open-source datasets, along with their Chinese translation versions, totaling approximately 12M samples.</td></tr>
                          <tr><th>Trainable Module</th><td colspan="2">ViT + MLP + LLM</td></tr>
                        </table>

                        <h3>Training (SFT)</h3>

                        <p>We provide <a href="https://github.com/OpenGVLab/InternVL/blob/main/internvl_chat/shell/internvl1.5/hermes2_yi34b/internvl_chat_v1_5_hermes2_yi34b_dynamic_res_finetune.sh">slurm scripts</a> for multi-node multi-GPU training. You can use either 32 or 64 GPUs to train this model. If you use 64 GPUs, training will take approximately 18 hours.</p>


                        <p>The hyperparameters used for fine-tuning are listed in the following table.</p>

                        <table>
                            <thead>
                                <tr>
                                    <th>Size</th>
                                    <th>Stage</th>
                                    <th>Trainable Module</th>
                                    <th>#Sample</th>
                                    <th>Drop Path</th>
                                    <th>Batch Size</th>
                                    <th>LR</th>
                                    <th>Epoch</th>
                                    <th>Max Length</th>
                                    <th>Weight Decay</th>
                                    <th>Config</th>
                                    <th>Download</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td rowspan="2">40B</td>
                                    <td>Pretrain</td>
                                    <td>ViT + MLP</td>
                                    <td>39.3M</td>
                                    <td>0.2</td>
                                    <td>8192</td>
                                    <td>2e-5</td>
                                    <td>1</td>
                                    <td>384</td>
                                    <td>0.05</td>
                                    <td>-</td>
                                    <td><a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-2">ViT</a> / <a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-2/blob/main/mlp_projector/hermes_2_yi_34b.pth">MLP</a></td>
                                </tr>
                                <tr>
                                    <td>Finetune</td>
                                    <td>ViT + MLP + LLM</td>
                                    <td>1.2M</td>
                                    <td>0.4</td>
                                    <td>512</td>
                                    <td>1e-5</td>
                                    <td>1</td>
                                    <td>2048</td>
                                    <td>0.05</td>
                                    <td><a href="https://github.com/OpenGVLab/InternVL/blob/main/internvl_chat/shell/internvl1.5/hermes2_yi34b/internvl_chat_v1_5_hermes2_yi34b_dynamic_res_finetune.sh">Link</a></td>
                                    <td><a href="https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2">MLLM</a></td>
                                </tr>
                            </tbody>
                        </table>

                        <h3>Performance</h3>

                        <p>* Proprietary Model</p>

                        <table>
                            <thead>
                                <tr>
                                    <th>name</th>
                                    <th>image size</th>
                                    <th>MMMU<br>(val)</th>
                                    <th>MMMU<br>(test)</th>
                                    <th>MathVista<br>(testmini)</th>
                                    <th>MMB<br>(test)</th>
                                    <th>MMB-CN<br>(test)</th>
                                    <th>MMVP</th>
                                    <th>MME</th>
                                    <th>SQA<br>(image)</th>
                                    <th>POPE</th>
                                    <th>TextVQA<br>(val)</th>
                                    <th>SEED<br>(image)</th>
                                    <th>VizWiz<br>(test)</th>
                                    <th>GQA<br>(test)</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>GPT-4V*</td>
                                    <td>unknown</td>
                                    <td>56.8</td>
                                    <td>55.7</td>
                                    <td>49.9</td>
                                    <td>77.0</td>
                                    <td>74.4</td>
                                    <td>38.7</td>
                                    <td>1409/517</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>78.0</td>
                                    <td>71.6</td>
                                    <td>-</td>
                                    <td>-</td>
                                </tr>
                                <tr>
                                    <td>Gemini Ultra*</td>
                                    <td>unknown</td>
                                    <td>59.4</td>
                                    <td>-</td>
                                    <td>53.0</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>82.3</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                </tr>
                                <tr>
                                    <td>Gemini Pro*</td>
                                    <td>unknown</td>
                                    <td>47.9</td>
                                    <td>-</td>
                                    <td>45.2</td>
                                    <td>73.6</td>
                                    <td>74.3</td>
                                    <td>40.7</td>
                                    <td>1497/437</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>74.6</td>
                                    <td>70.7</td>
                                    <td>-</td>
                                    <td>-</td>
                                </tr>
                                <tr>
                                    <td>Qwen-VL-Plus*</td>
                                    <td>unknown</td>
                                    <td>45.2</td>
                                    <td>40.8</td>
                                    <td>43.3</td>
                                    <td>67.0</td>
                                    <td>70.7</td>
                                    <td>-</td>
                                    <td>1681/502</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>78.9</td>
                                    <td>65.7</td>
                                    <td>-</td>
                                    <td>-</td>
                                </tr>
                                <tr>
                                    <td>Qwen-VL-Max*</td>
                                    <td>unknown</td>
                                    <td>51.4</td>
                                    <td>46.8</td>
                                    <td>51.0</td>
                                    <td>77.6</td>
                                    <td>75.7</td>
                                    <td>-</td>
                                    <td>1790/644</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>79.5</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                </tr>
                                <tr>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                </tr>
                                <tr>
                                    <td>LLaVA-NEXT-34B</td>
                                    <td>672x672</td>
                                    <td>51.1</td>
                                    <td>44.7</td>
                                    <td>46.5</td>
                                    <td>79.3</td>
                                    <td>79.0</td>
                                    <td>-</td>
                                    <td>1631/397</td>
                                    <td>81.8</td>
                                    <td>87.7</td>
                                    <td>69.5</td>
                                    <td>75.9</td>
                                    <td>63.8</td>
                                    <td>67.1</td>
                                </tr>
                                <tr  style="background-color: rgb(255,248,227);">
                                    <td><nobr>InternVL-Chat-V1-2</nobr></td>
                                    <td>448x448</td>
                                    <td>51.6</td>
                                    <td>46.2</td>
                                    <td>47.7</td>
                                    <td>82.2</td>
                                    <td>81.2</td>
                                    <td>56.7</td>
                                    <td>1687/489</td>
                                    <td>83.3</td>
                                    <td>88.0</td>
                                    <td>72.5</td>
                                    <td>75.6</td>
                                    <td>60.0</td>
                                    <td>64.0</td>
                                </tr>
                            </tbody>
                        </table>

                        <ul>
                            <li>MMBench results are collected from the <a href="https://mmbench.opencompass.org.cn/leaderboard">leaderboard</a>.</li>
                            <li>In most benchmarks, InternVL-Chat-V1-2 achieved better performance than LLaVA-NeXT-34B.</li>
                            <li>Update (2024-04-21): We have fixed a bug in the evaluation code of TextVQA, and the result has been corrected to 72.5.</li>
                        </ul>

                        <h2>InternVL-Chat-V1-2-Plus</h2>

                        <p><a href="https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2-Plus">InternVL-Chat-V1-2-Plus</a> uses the same model architecture as InternVL-Chat-V1-2, but the difference lies in the SFT dataset. InternVL-Chat-V1-2 only utilizes an SFT dataset with 1.2M samples, while our plus version employs an SFT dataset with 12M samples.</p>


                        <h3>Performance</h3>

                        <p>* Proprietary Model &nbsp;&nbsp;&nbsp;&nbsp; ‚Ä† Training Set Observed</p>

                        <table>
                            <thead>
                                <tr>
                                    <th>name</th>
                                    <th>image size</th>
                                    <th>MMMU<br>(val)</th>
                                    <th>MMMU<br>(test)</th>
                                    <th>MathVista<br>(testmini)</th>
                                    <th>MMB<br>(test)</th>
                                    <th>MMB-CN<br>(test)</th>
                                    <th>MMVP</th>
                                    <th>MME</th>
                                    <th>SQA<br>(image)</th>
                                    <th>POPE</th>
                                    <th>TextVQA<br>(val)</th>
                                    <th>SEED<br>(image)</th>
                                    <th>VizWiz<br>(test)</th>
                                    <th>GQA<br>(test)</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>GPT-4V*</td>
                                    <td>unknown</td>
                                    <td>56.8</td>
                                    <td>55.7</td>
                                    <td>49.9</td>
                                    <td>77.0</td>
                                    <td>74.4</td>
                                    <td>38.7</td>
                                    <td>1409/517</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>78.0</td>
                                    <td>71.6</td>
                                    <td>-</td>
                                    <td>-</td>
                                </tr>
                                <tr>
                                    <td>Gemini Ultra*</td>
                                    <td>unknown</td>
                                    <td>59.4</td>
                                    <td>-</td>
                                    <td>53.0</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>82.3</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                </tr>
                                <tr>
                                    <td>Gemini Pro*</td>
                                    <td>unknown</td>
                                    <td>47.9</td>
                                    <td>-</td>
                                    <td>45.2</td>
                                    <td>73.6</td>
                                    <td>74.3</td>
                                    <td>40.7</td>
                                    <td>1497/437</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>74.6</td>
                                    <td>70.7</td>
                                    <td>-</td>
                                    <td>-</td>
                                </tr>
                                <tr>
                                    <td>Qwen-VL-Plus*</td>
                                    <td>unknown</td>
                                    <td>45.2</td>
                                    <td>40.8</td>
                                    <td>43.3</td>
                                    <td>67.0</td>
                                    <td>70.7</td>
                                    <td>-</td>
                                    <td>1681/502</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>78.9</td>
                                    <td>65.7</td>
                                    <td>-</td>
                                    <td>-</td>
                                </tr>
                                <tr>
                                    <td>Qwen-VL-Max*</td>
                                    <td>unknown</td>
                                    <td>51.4</td>
                                    <td>46.8</td>
                                    <td>51.0</td>
                                    <td>77.6</td>
                                    <td>75.7</td>
                                    <td>-</td>
                                    <td>1790/644</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>79.5</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                </tr>
                                <tr>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                </tr>
                                <tr>
                                    <td>LLaVA-NEXT-34B</td>
                                    <td>672x672</td>
                                    <td>51.1</td>
                                    <td>44.7</td>
                                    <td>46.5</td>
                                    <td>79.3</td>
                                    <td>79.0</td>
                                    <td>-</td>
                                    <td>1631/397</td>
                                    <td>81.8</td>
                                    <td>87.7</td>
                                    <td>69.5</td>
                                    <td>75.9</td>
                                    <td>63.8</td>
                                    <td>67.1‚Ä†</td>
                                </tr>
                                <tr>
                                    <td>InternVL-Chat-V1-2</td>
                                    <td>448x448</td>
                                    <td>51.6</td>
                                    <td>46.2</td>
                                    <td>47.7</td>
                                    <td>82.2</td>
                                    <td>81.2</td>
                                    <td>56.7</td>
                                    <td>1687/489</td>
                                    <td>83.3</td>
                                    <td>88.0</td>
                                    <td>72.5</td>
                                    <td>75.6</td>
                                    <td>60.0</td>
                                    <td>64.0‚Ä†</td>
                                </tr>
                                <tr style="background-color: rgb(255,248,227);">
                                    <td><nobr>InternVL-Chat-V1-2-Plus</nobr></td>
                                    <td>448x448</td>
                                    <td>50.3</td>
                                    <td>45.6</td>
                                    <td>59.9</td>
                                    <td>83.8</td>
                                    <td>82.0</td>
                                    <td>58.7</td>
                                    <td>1625/553</td>
                                    <td>98.1‚Ä†</td>
                                    <td>88.7</td>
                                    <td>74.1‚Ä†</td>
                                    <td>76.4</td>
                                    <td>59.5</td>
                                    <td>66.9‚Ä†</td>
                                </tr>
                            </tbody>
                        </table>

                        <ul>
                            <li>MMBench results are collected from the <a href="https://mmbench.opencompass.org.cn/leaderboard">leaderboard</a>.</li>
                            <li>Update (2024-04-21): We have fixed a bug in the evaluation code of TextVQA, and the results have been corrected to 74.1.</li>
                        </ul>

                        <h3 class="title">Citation</h3>
<pre><code>
  @article{chen2023internvl,
      title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks},
      author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and Li, Bin and Luo, Ping and Lu, Tong and Qiao, Yu and Dai, Jifeng},
      journal={arXiv preprint arXiv:2312.14238},
      year={2023}
  }
  @article{chen2024far,
    title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},
    author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
    journal={arXiv preprint arXiv:2404.16821},
    year={2024}
  }
  </code></pre>
                        <br><h4 class="title"><a href="../">üîô Go Back</a></h4>
</div>


                </article>
            </div>
        </main>
        <footer class="site-footer h-card">
        <!--   <data class="u-url" href="https://internvl.github.io/blog/">InternVL</data>--> 
            <p class="footer-colophon">
            </p>
        </footer>
    </body>
</html>
