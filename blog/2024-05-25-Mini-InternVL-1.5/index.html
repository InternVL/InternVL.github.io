<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <!-- Begin Jekyll SEO tag v2.8.0 -->
        <title>Mini-InternVL 1.5: Super Small Model But with Competitive Performance</title>
        <meta name="generator" content="Jekyll v3.9.4"/>
        <meta name="author" content="chenzhe"/>
        <meta property="og:locale" content="en_US"/>
        <meta property="og:site_name" content="InternVL"/>
        <meta property="og:type" content="article"/>
        <meta property="article:published_time" content="2024-01-30T12:33:38-06:00"/>
        <meta name="twitter:card" content="summary"/>
        <!-- End Jekyll SEO tag -->
        <link rel="stylesheet" href="/blog/assets/main.css">
        <link type="application/atom+xml" rel="alternate" href="https://llava-vl.github.io/blog/feed.xml" title="LLaVA"/>
    </head>
    <body>
        <header class="site-header" role="banner">
            <div class="wrapper">
                <a class="site-title" rel="author" href="/blog/">InternVL</a>
                <nav class="site-nav">
                    <input type="checkbox" id="nav-trigger" class="nav-trigger"/>
                    <label for="nav-trigger">
                        <span class="menu-icon">
                            <svg viewBox="0 0 18 15" width="18px" height="15px">
                                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
                            </svg>
                        </span>
                    </label>
                    <div class="trigger"></div>
                </nav>
            </div>
        </header>
        <main class="page-content" aria-label="Content">
            <div class="wrapper">
                <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
                    <header class="post-header">
                        <h1 class="post-title p-name" itemprop="name headline">Mini-InternVL 1.5: Super Small Model But with Competitive Performance</h1>
                        <p class="post-meta">
                            <time class="dt-published" datetime="2024-05-20" itemprop="datePublished">May 20, 2024</time>
                            • 
                            <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                                <span class="p-author h-card" itemprop="name">Zhe Chen*, Zhangwei Gao*, Erfei Cui*, Wenhai Wang*, Weiyun Wang, Jinguo Zhu, Hao Tian, Xizhou Zhu, Lewei Lu, Tong Lu, Yu Qiao, Jifeng Dai (* Equal Contribution)</span>
                            </span>
                        </p>
                    </header>
                    <table>
                        <thead>
                        <tr>
                        <th>Type</th>
                        <th>Model</th>
                        <th>Date</th>
                        <th>Download</th>
                        <th>Note</th>
                        </tr>
                        </thead>
                        <tbody>

                        <tr>
                        <td rowspan="1">Vision Large Language Model</td>
                        <td>Mini-InternVL-Chat-2B-V1-5</td>
                        <td>2024.05.19</td>
                        <td>🤗 <a href="https://huggingface.co/OpenGVLab/Mini-InternVL-Chat-2B-V1-5" rel="nofollow">HF link</a></td>
                        <td>🚀🚀 Only 2B parameters, anyone can deploy it locally.</td>
                        </tr>
                        <tr>
                         <td>Vision Foundation Model</td>
                            <td>InternViT−300M−448px</td>
                        <td>2024.05.25</td>
                        <td>🤗 <a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px" rel="nofollow">HF link</a></td>
                        <td>Distilled small vision foundation model with 300M parameters.</td>
                        </tr>

                        </tbody>
                    </table>
                    <div class="post-content e-content" itemprop="articleBody">
                        <p>We are delighted to introduce <a href="https://huggingface.co/OpenGVLab/Mini-InternVL-Chat-2B-V1-5">Mini-InternVL-Chat-2B-V1-5</a>. In the era of large language models, many researchers have started to focus on smaller language models, such as Gemma-2B, Qwen-1.8B, and InternLM2-1.8B. Inspired by their efforts, we have distilled our vision foundation model <a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5">InternViT-6B-448px-V1-5</a> down to 300M and used <a href="https://huggingface.co/internlm/internlm2-chat-1_8b">InternLM2-Chat-1.8B</a> as our language model. This resulted in a small multimodal model with excellent performance.</p>

                        <p>As shown in the figure below, we adopted the same model architecture as InternVL 1.5. We simply replaced the original InternViT-6B with InternViT-300M and InternLM2-Chat-20B with InternLM2-Chat-1.8B. For training, we used the same data as InternVL 1.5 to train this smaller model. Additionally, due to the lower training costs of smaller models, <b>we used a context length of 8K during training.</b></p>
                        <img width="40%" alt="image" src="images/mini_internvl_architecture.png"><br>
                        <p>From the experimental results, we've observed that our distilled small vision model (InternViT-300M) is well-suited for a smaller language model (1.8B). This combination maximizes efficiency while maintaining impressive performance across various benchmarks, demonstrating the effectiveness of small models in handling complex tasks. Additionally, our small model significantly reduces memory requirements, making it more accessible and efficient for practical use.</p>

                        <h3>Performance</h3>

                        <table class="performance">

                            <thead>
                                <tr>
                                    <th>model</th>
                                    <th>open-<br>source</th>
                                    <th>#param</th>
                                    <th>DocVQA<br>(test)</th>
                                    <th>ChartQA<br>(test)</th>
                                    <th>InfoVQA<br>(test)</th>
                                    <th>TextVQA<br>(val)</th>
                                    <th>OCR<br>Bench</th>
                                    <th>MME</th>
                                    <th>RWQA</th>
                                    <th>AI2D<br>(test)</th>
                                    <th>MMMU<br>(val)</th>
                                    <th>MMB−EN/CN<br>(test)</th>
                                    <th>CCB<br>(dev)</th>
                                    <th>MMVet</th>
                                    <th>SEED<br>(image)</th>
                                    <th>HallB</th>
                                    <th>MathVista<br>(mini)</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr style="background-color: rgba(117, 209, 215, 0.1);">
                                    <td>GPT-4V</td>
                                    <td>✗</td>
                                    <td>-</td>
                                    <td>88.4</td>
                                    <td>78.5</td>
                                    <td>-</td>
                                    <td>78.0</td>
                                    <td>645</td>
                                    <td>1926.6</td>
                                    <td>61.4</td>
                                    <td>78.2</td>
                                    <td>56.8</td>
                                    <td>77.0 / 74.4</td>
                                    <td>46.5</td>
                                    <td>67.6</td>
                                    <td>71.6</td>
                                    <td>46.5</td>
                                    <td>49.9</td>
                                </tr>

                                <tr style="background-color: rgba(117, 209, 215, 0.1);">
                                    <td>Gemini Pro 1.0</td>
                                    <td>✗</td>
                                    <td>-</td>
                                    <td>88.1</td>
                                    <td>74.1</td>
                                    <td>75.2</td>
                                    <td>74.6</td>
                                    <td>659</td>
                                    <td>1933.4</td>
                                    <td>-</td>
                                    <td>73.9</td>
                                    <td>47.9</td>
                                    <td>73.6 / 74.3</td>
                                    <td>52.5</td>
                                    <td>64.3</td>
                                    <td>70.7</td>
                                    <td>45.2</td>
                                    <td>45.2</td>
                                </tr>
                                <tr style="background-color: rgba(117, 209, 215, 0.1);">
                                    <td>Gemini Pro 1.5</td>
                                    <td>✗</td>
                                    <td>-</td>
                                    <td>86.5</td>
                                    <td>81.3</td>
                                    <td>72.7</td>
                                    <td>73.5</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>67.5</td>
                                    <td>80.3</td>
                                    <td>58.5</td>
                                    <td>-/-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>52.1</td>
                                </tr>
                                <tr style="background-color: rgba(117, 209, 215, 0.1);">
                                    <td>Qwen-VL-Plus</td>
                                    <td>✗</td>
                                    <td>-</td>
                                    <td>91.4</td>
                                    <td>78.1</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>694</td>
                                    <td>2183.4</td>
                                    <td>-</td>
                                    <td>75.9</td>
                                    <td>45.2</td>
                                    <td>67.0/70.7</td>
                                    <td>55.1</td>
                                    <td>61.1</td>
                                    <td>72.7</td>
                                    <td>40.6</td>
                                    <td>43.3</td>
                                </tr>
                                <tr style="background-color: rgba(117, 209, 215, 0.1);">
                                    <td>Claude-3 Haiku</td>
                                    <td>✗</td>
                                    <td>-</td>
                                    <td>88.8</td>
                                    <td>81.7</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>658</td>
                                    <td>1453.2</td>
                                    <td>-</td>
                                    <td>86.7</td>
                                    <td>50.2</td>
                                    <td>60.7 / 57.2</td>
                                    <td>24.5</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>39.2</td>
                                    <td>46.4</td>
                                </tr>

                                <tr style="background-color: rgba(117, 209, 215, 0.1);">
                                    <td>Step-1V</td>
                                    <td>✗</td>
                                    <td>100B</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>625</td>
                                    <td>2206.4</td>
                                    <td>-</td>
                                    <td>79.2</td>
                                    <td>49.9</td>
                                    <td>80.7 / 79.9</td>
                                    <td>71.2</td>
                                    <td>63.3</td>
                                    <td>70.3</td>
                                    <td>48.4</td>
                                    <td>44.8</td>
                                </tr>
                                <tr style="background-color: rgba(117, 209, 215, 0.1);">
                                    <td>Grok-1.5V</td>
                                    <td>✗</td>
                                    <td>-</td>
                                    <td>85.6</td>
                                    <td>76.1</td>
                                    <td>-</td>
                                    <td>78.1</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>68.7</td>
                                    <td>88.3</td>
                                    <td>-</td>
                                    <td>-/-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>52.8</td>
                                </tr>
                                <tr style="background-color: rgba(249, 242, 248, 1);">
                                    <td>LLaVA-NeXT−34B</td>
                                    <td>✓</td>
                                    <td>35B</td>
                                    <td>84.0</td>
                                    <td>68.7</td>
                                    <td>51.5</td>
                                    <td>69.5</td>
                                    <td>574</td>
                                    <td>2028.0</td>
                                    <td>-</td>
                                    <td>74.9</td>
                                    <td>51.1</td>
                                    <td>81.1 / 79.0</td>
                                    <td>49.2</td>
                                    <td>57.4</td>
                                    <td>75.9</td>
                                    <td>34.8</td>
                                    <td>46.5</td>
                                </tr>
                                <tr style="background-color: rgba(249, 242, 248, 1);">
                                    <td>MiniCPM-V-2</td>
                                    <td>✓</td>
                                    <td>2.8B</td>
                                    <td>71.9</td>
                                    <td></td>
                                    <td></td>
                                    <td>74.1</td>
                                    <td>605</td>
                                    <td>1808.6</td>
                                    <td>55.8</td>
                                    <td>62.9</td>
                                    <td>38.2</td>
                                    <td>69.6/68.1</td>
                                    <td></td>
                                    <td>41</td>
                                    <td>67.1</td>
                                    <td>36.1</td>
                                    <td>38.7</td>
                                </tr>
                                <tr style="background-color: rgba(249, 242, 248, 1);">
                                    <td>PaliGemma-3B</td>
                                    <td>✓</td>
                                    <td>3B</td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td>614</td>
                                    <td>1686.1</td>
                                    <td>55.2</td>
                                    <td>68.3</td>
                                    <td>34.9</td>
                                    <td></td>
                                    <td></td>
                                    <td>33.1</td>
                                    <td>69.6</td>
                                    <td>32.2</td>
                                    <td>28.7</td>
                                </tr>
                                <tr style="background-color: rgba(249, 242, 248, 1);">
                                    <td>MiniCPM-V</td>
                                    <td>✓</td>
                                    <td>2.8B</td>
                                    <td>38.2</td>
                                    <td></td>
                                    <td></td>
                                    <td>60.6</td>
                                    <td>366</td>
                                    <td>1452</td>
                                    <td>51.2</td>
                                    <td>56.3</td>
                                    <td>38.3</td>
                                    <td>67.9/65.3</td>
                                    <td>65.6</td>
                                    <td>31.1</td>
                                    <td></td>
                                    <td>36.2</td>
                                    <td>28.9</td>
                                </tr>
                                <tr style="background-color: rgba(249, 242, 248, 1);">
                                    <td>DeepSeek−VL−1.3B</td>
                                    <td>✓</td>
                                    <td>1.3B</td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td></td>
                                    <td>409</td>
                                    <td>1531.6</td>
                                    <td>49.7</td>
                                    <td>51.5</td>
                                    <td>32.2</td>
                                    <td></td>
                                    <td></td>
                                    <td>34.8</td>
                                    <td>66.7</td>
                                    <td>27.6</td>
                                    <td>31.1</td>
                                </tr>
                                <tr style="background-color: rgba(249, 242, 248, 1);">
                                    <td>InternVL 1.2</td>
                                    <td>✓</td>
                                    <td>40B</td>
                                    <td>57.7</td>
                                    <td>68.0</td>
                                    <td>39.5</td>
                                    <td>72.5</td>
                                    <td>569</td>
                                    <td>2175.4</td>
                                    <td>67.5</td>
                                    <td>79.0</td>
                                    <td>51.6</td>
                                    <td>82.2 / 81.2</td>
                                    <td>59.2</td>
                                    <td>48.9</td>
                                    <td>75.6</td>
                                    <td>47.6</td>
                                    <td>47.7</td>
                                </tr>
                                <tr style="background-color: rgba(249, 242, 248, 1);">
                                    <td>InternVL 1.5</td>
                                    <td>✓</td>
                                    <td>25.5B</td>
                                    <td>90.9</td>
                                    <td>83.8</td>
                                    <td>72.5</td>
                                    <td>80.6</td>
                                    <td>724</td>
                                    <td>2187.8</td>
                                    <td>66.0</td>
                                    <td>80.7</td>
                                    <td>45.2</td>
                                    <td>82.2 / 82.0</td>
                                    <td>69.8</td>
                                    <td>62.8</td>
                                    <td>76.0</td>
                                    <td>49.3</td>
                                    <td>53.5</td>
                                </tr>
                                <tr style="background-color: rgb(255,248,227);">
                                    <td>Mini−InternVL&nbsp;1.5</td>
                                    <td>✓</td>
                                    <td>2.2B</td>
                                    <td>82.6</td>
                                    <td>74.6</td>
                                    <td>56.0</td>
                                    <td>70.7</td>
                                    <td>654</td>
                                    <td>1902.3</td>
                                    <td>57.9</td>
                                    <td>70.0</td>
                                    <td>33.6</td>
                                    <td>70.7 / 65.6</td>
                                    <td>63.5</td>
                                    <td>38.4</td>
                                    <td>69.8</td>
                                    <td>37.5</td>
                                    <td>41.1</td>
                                </tr>
                                <tr style="background-color: rgb(255,248,227);">
                                    <td colspan="2"><b>Percent of InternVL 1.5</b></td>
                                    <td><b>8.6%</b></td>
                                    <td><b>90.9%</b></td>
                                    <td><b>89.0%</b></td>
                                    <td><b>77.2%</b></td>
                                    <td><b>87.6%</b></td>
                                    <td><b>90.3%</b></td>
                                    <td><b>87.0%</b></td>
                                    <td><b>87.7%</b></td>
                                    <td><b>86.7%</b></td>
                                    <td><b>74.3%</b></td>
                                    <td><b>83.0%</b></td>
                                    <td><b>91.0%</b></td>
                                    <td><b>61.1%</b></td>
                                    <td><b>91.8%</b></td>
                                    <td><b>76.1%</b></td>
                                    <td><b>76.8%</b></td>
                                </tr>
                            </tbody>
                        </table>
                        <style>
                                .performance table {
                                    font-size: 16px; /* 设置整个表格的字体大小 */
                                }
                                .performance th {
                                    font-size: 14px; /* 设置表头的字体大小 */
                                    padding: 6px!important;  /* 较小的内边距值 */
                                }
                                .performance td {
                                    font-size: 14px; /* 设置表格单元格的字体大小 */
                                    padding: 6px!important;;  /* 较小的内边距值 */
                                }
                            </style>

                        <h3 id="model-card">Model Card</h3>

                        <table  style="text-align: center;">
                          <tr><th colspan="2">Name</th><th>Mini-InternVL-Chat-2B-V1-5</th></tr>
                          <tr><th rowspan="4">Model Size</th><td>Total</td><td><b>2.21B</b></td></tr>
                          <tr><td>ViT</td><td>304.01M</td></tr>
                          <tr><td>MLP</td><td>12.60M</td></tr>
                          <tr><td>LLM</td><td>1.89B</td></tr>
                          <tr><th colspan="1">Resolution</th><td colspan="2" >dynamic resolution, max to 12 tiles of 448 × 448 in training, max to 40 tiles in testing (4K resolution).</td></tr>
                            <tr><th rowspan="2">Stage-1</th><th>Training Data</th><td colspan="1"  style="text-align: left;">The pre-training dataset utilized in our InternVL 1.5 encompasses a diverse range of publicly accessible sources. These datasets span multiple tasks, including captioning, which predominantly uses datasets such as Laion-EN, Laion-ZH, COYO, and GRIT, constituting 53.9% of the total data. Detection and grounding tasks utilize datasets like Objects365, GRIT, and All-Seeing, making up 5.2%. For OCR tasks, we utilized large-scale datasets such as Wukong-OCR, LaionCOCO-OCR, and Common Crawl PDFs, which constitute 32.0% of our data. These datasets were constructed using PaddleOCR to perform OCR on Chinese images from Wukong and on English images from LaionCOCO. Smaller OCR datasets include MMC-Inst, LSVT, ST-VQA, RCTW-17, ArT, and others, accounting for 8.9% of the data, which focus on more specific or constrained OCR challenges.</td></tr>
                          <tr><th>Trainable Module</th><td colspan="1">ViT + MLP</td></tr>
                          <tr><th rowspan="2">Stage-2</th><th>Training Data</th><td colspan="1">5M high-quality bilingual data. Please see the technical report of InternVL 1.5 for more details.</td></tr>
                          <tr><th>Trainable Module</th><td colspan="2">ViT + MLP + LLM</td></tr>
                        </table>

                        <p>The hyperparameters used for pre-training and fine-tuning are listed in the following table.</p>

                        <table>
                            <thead>
                                <tr>
                                    <th>Stage</th>
                                    <th>Trainable Module</th>
                                    <th>#Samples</th>
                                    <th>Batch Size</th>
                                    <th>Learning rate</th>
                                    <th>Epoch</th>
                                    <th>Max length</th>
                                    <th>Weight decay</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Pre-train</td>
                                    <td>ViT + MLP</td>
                                    <td>~200M</td>
                                    <td>2048</td>
                                    <td>2e-5</td>
                                    <td>1</td>
                                    <td>4096</td>
                                    <td>0.01</td>
                                </tr>
                                <tr>
                                    <td>Fine-tune</td>
                                    <td>ViT + MLP + LLM (full model)</td>
                                    <td>~5M</td>
                                    <td>1024</td>
                                    <td>4e-5</td>
                                    <td>1</td>
                                    <td>8192</td>
                                    <td>0.01</td>
                                </tr>
                            </tbody>
                        </table>

                    </div>
                    <h2 class="title">Citation</h2>
<pre><code>
  @article{chen2023internvl,
      title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks},
      author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and Li, Bin and Luo, Ping and Lu, Tong and Qiao, Yu and Dai, Jifeng},
      journal={arXiv preprint arXiv:2312.14238},
      year={2023}
  }
  @article{chen2024far,
    title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},
    author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
    journal={arXiv preprint arXiv:2404.16821},
    year={2024}
  }
  </code></pre>
                </article>
            </div>
        </main>
        <footer class="site-footer h-card">
            <data class="u-url" href="/blog/"></data>
            <div class="wrapper">
                <p class="footer-colophon">
                </p>
            </div>
        </footer>
    </body>
</html>
