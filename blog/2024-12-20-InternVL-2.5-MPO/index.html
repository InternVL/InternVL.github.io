<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>InternVL2.5</title>
    <link rel="icon" href="https://github.com/OpenGVLab/InternVL/assets/47669167/7037290e-f474-4d11-b90f-1d8316087bf8">
    <meta name="description" content="...">
    <link rel="stylesheet" href="/blog/assets/main.css">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <!-- <link rel="stylesheet" href="/static/css/bulma.min.css"> -->
    <link rel="stylesheet" href="/static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="/static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <style>
        .results-carousel {
            overflow: hidden;
        }
    </style>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="/static/js/bulma-carousel.min.js"></script>
    <script src="/static/js/bulma-slider.min.js"></script>
    <script src="/static/js/explorer-index.js"></script>

    <link rel="stylesheet" type="text/css" href="https://at.alicdn.com/t/font_1582902_u0zm91pv15i.css">
    <style type="text/css">
        body {
            margin: 0;
            padding: 0;
        }

        #carousel {
            margin: auto;
            width: 100%;
            /* 使用相对宽度 */
            /* max-width: 600px; */
            /* 设置最大宽度 */
            position: relative;
            overflow: hidden;
            height: auto;
        }

        #carousel>ul {
            display: flex;
            position: relative;
            transition: left 0.5s ease;
            /* 添加过渡效果 */
            left: 0;
            /* 初始位置 */
        }

        #carousel>ul,
        #carousel>ul>li {
            padding: 0;
            margin: 0;
            list-style: none;
            width: 100%;
            /* 使每个li宽度相对于#carousel */
            /* n张图就是n*100%，下面js代码里会动态设置 */
        }

        #carousel img {
            width: 100%;
            /* 使图片宽度相对于li的宽度 */
        }

        .arrow {
            position: absolute;
            top: 50%;
            transform: translateY(-50%);
            font-size: 30px;
            cursor: pointer;
            color: #fff;
            /* 箭头颜色 */
            background: rgba(0, 0, 0, 0.5);
            /* 箭头背景 */
            border-radius: 50%;
            padding: 10px;
            z-index: 1000;
            /* 确保箭头在最上层 */
        }

        .left-arrow {
            left: 2%;
        }

        .right-arrow {
            right: 2%;
        }
    </style>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.js"></script>
</head>

<body>
    <header class="site-header" role="banner">
        <div class="wrapper">
            <a class="site-title" rel="author" href="/">InternVL</a>
            <nav class="site-nav">
                <input type="checkbox" id="nav-trigger" class="nav-trigger" />
                <label for="nav-trigger">
                    <span class="menu-icon">
                        <svg viewBox="0 0 18 15" width="18px" height="15px">
                            <path
                                d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z" />
                        </svg>
                    </span>
                </label>
                <div class="trigger"></div>
            </nav>
        </div>
    </header>
    <main class="page-content" aria-label="Content">
        <style>
            @media (max-width: 768px) {
                img.responsive {
                    width: 100% !important;
                }
            }

            .no-alternate tr {
                background-color: white !important;
            }
        </style>
        <div class="wrapper">
            <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
                <header class="post-header">
                    <h1 class="post-title p-name" itemprop="name headline">
                        InternVL2.5-MPO: Enhancing the Reasoning Ability of Multimodal Large Language Models via
                        Mixed Preference Optimization
                    </h1>
                    <p class="post-meta">
                        <time class="dt-published" datetime="2024-04-30" itemprop="datePublished">2024/12/20</time>
                        •
                        <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                            <span class="p-author h-card" itemprop="name">OpenGVLab Team</span>
                        </span>
                    </p>
                </header>
                <p>
                    <a rel="nofollow" href="../">[🆕 Go Back]</a>
                    <a rel="nofollow" href="https://huggingface.co/papers/2411.10442">[📜 InternVL2.5-MPO Paper]</a>
                    <a rel="nofollow" href="https://huggingface.co/papers/2412.05271">[📜 InternVL2.5 Report]</a>
                    <a rel="nofollow"
                        href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_InternVL_Scaling_up_Vision_Foundation_Models_and_Aligning_for_Generic_CVPR_2024_paper.pdf">
                        [📜 InternVL 1.0 Paper]</a>
                    <a rel="nofollow" href="https://arxiv.org/abs/2404.16821">[📜 InternVL1.5 Paper]</a>
                    <a rel="nofollow" href="https://github.com/OpenGVLab/InternVL/tree/main">[📂 GitHub]</a>
                    <a rel="nofollow" href="https://internvl.readthedocs.io/en/latest/">[📖 Documents]</a>
                    <br>
                    <a rel="nofollow" href="https://internvl.opengvlab.com/">[🗨️ Chat Demo]</a>
                    <a rel="nofollow" href="https://huggingface.co/spaces/OpenGVLab/InternVL">[🤗 HF Demo]</a>
                    <a rel="nofollow"
                        href="https://modelscope.cn/models/OpenGVLab/Mini-InternVL-Chat-4B-V1-5/summary">[<img
                            src="images/modelscope_logo.png" width="20px" style="max-width: 100%;"> ModelScope]</a>
                    <a rel="nofollow"
                        href="https://github.com/OpenGVLab/InternVL?tab=readme-ov-file#quick-start-with-huggingface">[🚀
                        Quick Start]</a>
                </p>

                <table>
                    <thead>
                        <tr>
                            <th>Model</th>
                            <th>Date</th>
                            <th>HF Link</th>
                            <th>MS Link</th>
                            <th>Document</th>
                            <th>OC Performance</th>
                        </tr>
                    </thead>
                    <tbody>

                        <tr>
                            <td>InternVL2_5-1B-MPO</td>
                            <td>2024.12.20</td>
                            <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-1B-MPO" rel="nofollow">🤗 link</a>
                            </td>
                            <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2_5-1B-MPO" rel="nofollow">🤖
                                    link</a>
                            </td>
                            <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-1B-MPO#quick-start"
                                    rel="nofollow">📖
                                    doc</a></td>
                            <!-- 54.9 -->
                            <td>56.4 <span style="color: green; font-weight: bold;">(+1.5)</span></td>
                        </tr>
                        <tr>
                            <td>InternVL2_5-2B-MPO</td>
                            <td>2024.12.20</td>
                            <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-2B-MPO" rel="nofollow">🤗 link</a>
                            </td>
                            <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2_5-2B-MPO" rel="nofollow">🤖
                                    link</a>
                            </td>
                            <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-2B-MPO#quick-start"
                                    rel="nofollow">📖
                                    doc</a></td>
                            <!-- 59.9 -->
                            <td>62.0 <span style="color: green; font-weight: bold;">(+2.1)</span></td>
                        </tr>
                        <tr>
                            <td>InternVL2_5-4B-MPO</td>
                            <td>2024.12.20</td>
                            <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-4B-MPO" rel="nofollow">🤗 link</a>
                            </td>
                            <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2_5-4B-MPO" rel="nofollow">🤖
                                    link</a>
                            </td>
                            <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-4-MPO#quick-start"
                                    rel="nofollow">📖
                                    doc</a></td>
                            <!-- 65.1 -->
                            <td>67.6 <span style="color: green; font-weight: bold;">(+2.5)</span></td>
                        </tr>
                        <tr>
                            <td>InternVL2_5-8B-MPO</td>
                            <td>2024.12.20</td>
                            <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-8B-MPO" rel="nofollow">🤗 link</a>
                            </td>
                            <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2_5-8B-MPO" rel="nofollow">🤖
                                    link</a>
                            </td>
                            <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-8B-MPO#quick-start"
                                    rel="nofollow">📖
                                    doc</a></td>
                            <!-- 68.1 -->
                            <td>70.4 <span style="color: green; font-weight: bold;">(+2.3)</span></td>
                        </tr>
                        <tr>
                            <td>InternVL2_5-26B-MPO</td>
                            <td>2024.12.20</td>
                            <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-26B-MPO" rel="nofollow">🤗
                                    link</a>
                            </td>
                            <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2_5-26B-MPO" rel="nofollow">🤖
                                    link</a></td>
                            <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-26B-MPO#quick-start"
                                    rel="nofollow">📖
                                    doc</a></td>
                            <!-- 71.6 -->
                            <td>72.9 <span style="color: green; font-weight: bold;">(+1.3)</span></td>
                        </tr>
                        <tr>
                            <td>InternVL2_5-38B-MPO</td>
                            <td>2024.12.20</td>
                            <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-38B-MPO" rel="nofollow">🤗
                                    link</a>
                            </td>
                            <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2_5-38B-MPO" rel="nofollow">🤖
                                    link</a></td>
                            <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-38B-MPO#quick-start"
                                    rel="nofollow">📖
                                    doc</a></td>
                            <!-- 73.5 -->
                            <td>75.5 <span style="color: green; font-weight: bold;">(+2.0)</span></td>
                        </tr>
                        <tr>
                            <td>InternVL2_5-78B-MPO</td>
                            <td>2024.12.20</td>
                            <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-78B-MPO" rel="nofollow">🤗
                                    link</a></td>
                            <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2_5-78B-MPO" rel="nofollow">🤖
                                    link</a></td>
                            <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-78B-MPO#quick-start"
                                    rel="nofollow">📖
                                    doc</a></td>
                            <!-- 75.2 -->
                            <td>77.2 <span style="color: green; font-weight: bold;">(+2.0)</span></td>
                        </tr>
                        <tr>
                            <td>InternVL2_5-Pro</td>
                            <td>-</a></td>
                            <td>-</a></td>
                            <td>-</a></td>
                            <td>-</a></td>
                            <td>79.1</a></td>
                        </tr>

                    </tbody>
                </table>
                <p>* OC performance refers to the results on the OpenCompass Leaderboard.</p>
                <p>
                    * The values in parentheses indicate the performance gap between the model after and before MPO.
                </p>


                <div class="post-content e-content" itemprop="articleBody">
                    <br>
                    <p>
                        We introduce InternVL2.5-MPO, an advanced multimodal large language model (MLLM) series that
                        demonstrates superior overall performance.
                        This series builds upon InternVL2.5 and Preference Optimization.

                        Existing open-source multimodal large language models (MLLMs) generally follow a training
                        process
                        involving pre-training and supervised fine-tuning. However, these models suffer from
                        distribution shifts,
                        which limit their multimodal reasoning, particularly in the Chain-of-Thought (CoT) performance.

                        To address this, we introduce a preference optimization (PO) process to enhance the multimodal
                        reasoning
                        capabilities of MLLMs. Specifically, (1) on the data side, we design an automated preference
                        data
                        construction pipeline to create <a
                            href="https://huggingface.co/datasets/OpenGVLab/MMPR-v1.1">MMPR</a>, a
                        high-quality,
                        large-scale multimodal reasoning preference dataset. and (2) on the model side, we explore
                        integrating PO
                        with MLLMs, developing a simple yet effective method, termed Mixed Preference Optimization
                        (MPO), which boosts multimodal CoT performance.

                        Our approach demonstrates improved performance across multiple benchmarks, particularly in
                        multimodal reasoning tasks.

                        <b>
                            Notably, our models outperform their counterparts without MPO by an average of 2 points
                            across all scales on the OpenCompass leaderboard.
                        </b>

                        We hope this study could inspire further advancements in MLLMs.
                    </p>
                    <p>InternVL2.5-MPO family is built upon the following designs:</p>
                    <ol>
                        <li>
                            <b><u>M</u>ulti-<u>M</u>odal <u>Pr</u>eference Dataset (MMPR)</b>:
                            We propose an efficient preference data construction pipeline.
                            Based on this pipeline, we create MMPR, a high-quality, large-scale multimodal reasoning
                            preference dataset containing approximately 3 million samples.
                        </li>
                        <li>
                            <b><u>M</u>ixed <u>P</u>reference <u>O</u>ptimization (MPO)</b>:
                            We introduce MPO, an effective PO algorithm designed to improve the reasoning abilities of
                            MLLMs. The key insight behind this algorithm is that an effective PO process should enable
                            the model to learn the relative preference between pairs of responses, the absolute quality
                            of individual responses, and the process for generating preferred responses.
                        </li>
                    </ol>

                    <center>
                        <p><img class="responsive" width="90%" alt="image" src="images/overview_performance.png"></p>
                    </center>

                    <h3 id="performance">MMPR</h3>
                    <div>
                        <p>
                            To construct a large-scale preference optimization dataset, we propose an efficient data
                            construction
                            pipeline. Specifically, we categorize the multimodal data into <b>samples with clear ground
                                truths</b> and
                            <b>samples without clear ground truths</b>.

                        <ul type="1">
                            <li>
                                <b>For samples with clear ground truths</b>,
                                <span style="font-size: 95%;">
                                    the model is prompted to first provide the reasoning process and then give the final
                                    answer in the
                                    format like <i>"Final Answer: xxx"</i>. Responses matching the ground truth answer
                                    constitute the
                                    positive set <span class="MMPR-Positive-Set"></span>, while those
                                    that do not match make up the negative set <span class="MMPR-Negative-Set"></span>.
                                    Additionally,
                                    responses that fail to provide a clear final answer are also merged into
                                    <span class="MMPR-Negative-Set"></span>. Given these responses labeled as positive
                                    or
                                    negative, we
                                    build the preference pairs by selecting a chosen response from
                                    <span class="MMPR-Positive-Set"></span> and a negative response from
                                    <span class="MMPR-Negative-Set"></span>.
                                </span>
                            </li>
                            <li>
                                <b>For samples without clear ground truths</b>,
                                <span style="font-size: 95%;">
                                    we propose a simple yet effective method: Dropout Next-Token Prediction (Dropout
                                    NTP).
                                    Specifically, we use the responses generated by InternVL2-8B as chosen answers.
                                    Given the chosen answer, we truncate it by half and then prompt InternVL2-8B to
                                    complete
                                    the remaining
                                    portion of the truncated answer without access to the image input.
                                    This generated completion serves as the rejected answer for the paired sample.
                                    It is worth noting that while the responses generated by InternVL2-8B may not be
                                    perfect,
                                    the completions generated without the image input will introduce more hallucinations
                                    than those
                                    generated with the image input. Therefore, the partial order relationship between
                                    the
                                    chosen and
                                    rejected responses holds true.
                                </span>
                            </li>
                            <br>
                        </ul>
                        </p>
                    </div>
                    <div>
                        <div id="carousel">
                            <div class="arrow left-arrow" onclick="moveSlide(-1)">&#9664;</div>
                            <div class="arrow right-arrow" onclick="moveSlide(1)">&#9654;</div>
                            <ul> <!-- 图片容器 -->
                                <li>
                                    <img
                                        src="https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/mmXL47UPDFwYOWdn9Z6j5.jpeg">
                                </li>
                                <li>
                                    <img
                                        src="https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/6fnvI_wCd9JXAs6vYthaG.jpeg">
                                </li>
                                <li>
                                    <img
                                        src="https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/0zTSIITRWSayDhjLCVp51.jpeg">
                                </li>
                                <li>
                                    <img
                                        src="https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/eDuVqKVUXZo9NcMyyYJps.jpeg">
                                </li>
                                <li>
                                    <img
                                        src="https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/xnE-OGscFSiy3XZVXQUiN.jpeg">
                                </li>
                                <li>
                                    <img
                                        src="https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/syEYaoCH6NqpANHz9gPT7.jpeg">
                                </li>
                                <li>
                                    <img
                                        src="https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/WpGgpEgJqxwOsKZxbd3NR.jpeg">
                                </li>
                                <li>
                                    <img
                                        src="https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/GcaMGTc_1aeWGfTNzgS18.jpeg">
                                </li>
                                <li>
                                    <img
                                        src="https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/5uMvxy2KzIG5dv3HZtnaq.jpeg">
                                </li>
                                <!-- <li>
                                    <img
                                        src="https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/3TFfr5cPtJsw65xD08Ct_.jpeg">
                                </li> -->
                                <li>
                                    <img
                                        src="https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/a9GvpQv9nR1qhCe79sByV.jpeg">
                                </li>
                                <li>
                                    <img
                                        src="https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/Bwn6oMy7LGDZCQP3i5HJi.jpeg">
                                </li>

                            </ul>
                        </div>
                    </div>


                    <h3 id="performance">Mixed Preference Optimization</h3>
                    <div>
                        <p>
                            The key insight behind MPO is that
                            <b>
                                an effective PO process should enable the model to learn the
                                relative preference between pairs of responses, the absolute quality of individual
                                responses, and the
                                process for generating preferred responses.
                            </b>
                            We define the training objective as a combination of

                            preference loss <span id="pref-loss-left"></span>,
                            quality loss <span id="quality-loss-left"></span>,
                            and generation loss <span id="gen-loss-left"></span>,

                            referred to as Mixed Preference Optimization:
                        </p>

                        <div id="overall-loss" style="text-align: center"></div>
                        <p>
                            where <span id="w_i"></span> represents the weight assigned to each loss component.
                            In this work, we empirically compare different variants of preference loss.
                            Based on the experimental results, we use DPO as our preference loss and BCO as our quality
                            loss.
                        </p>

                        </p>
                        Specifically, the DPO serves as the preference loss to enable the model to learn the
                        relative preference between chosen and rejected responses.
                        This algorithm optimizes the following loss function:
                        </p>

                        <div id="pref-loss" style="text-align: center"></div>
                        <p>
                            where
                            <span id="beta" style="text-align: center"></span>
                            is the KL penalty coefficient,
                            and
                            <span id="x" style="text-align: center"></span>,
                            <span id="y_c" style="text-align: center"></span>,
                            and
                            <span id="y_r" style="text-align: center"></span>
                            are user query, chosen response, and rejected response, respectively.
                            The policy model
                            <span id="pi_theta" style="text-align: center"></span>
                            is initialized from model
                            <span id="pi_0" style="text-align: center"></span>.
                        </p>

                        <p>
                            Additionally, the BCO loss is employed as the quality loss, which helps the model to
                            understand the absolute quality of individual responses.
                            The loss function is defined as:
                        </p>
                        <div id="quality-loss" style="text-align: center"></div>

                        <p>
                            where <span id="quality-positive-loss-left" style="text-align: center"></span> and
                            <span id="quality-negative-loss-left" style="text-align: center"></span> represent the loss
                            for
                            chosen
                            and rejected responses, respectively.
                            Each response type's loss is calculated independently, requiring the model to differentiate
                            the
                            absolute
                            quality of individual responses. The loss terms are given by:
                        </p>
                        <div id="quality-positive-loss" style="text-align: center"></div>
                        <div id="quality-negative-loss" style="text-align: center"></div>

                        <p>
                            where
                            <span id="delta" style="text-align: center"></span>
                            represents the reward shift, calculated as the moving average of previous rewards to
                            stabilize training.
                        </p>

                        <p>
                            Finally, the SFT loss is used as the generation loss to help the model learn the generation
                            process of
                            preferred responses.
                            The loss function is defined as:
                        </p>

                        <div id="gen-loss" style="text-align: center"></div>
                    </div>


                    <h3 id="performance">Performance</h3>
                    <p>
                        To comprehensively compare InternVL's performance before and after MPO,
                        we employ the benchmarks from OpenCompass Learderboard,
                        including both well-established classic datasets and newly introduced ones.
                        These benchmarks span a wide range of categories, aiming to provide a thorough and balanced
                        assessment of InternVL’s capabilities across various multimodal tasks.
                        We provide the evaluation results in the tables behind.
                    </p>

                    <table>
                        <tr>
                            <th>Model</th>
                            <th>Avg.</th>
                            <th>MMBench v1.1</th>
                            <th>MMStar</th>
                            <th>MMMU</th>
                            <th>MathVista</th>
                            <th>HallusionBench</th>
                            <th>AI2D</th>
                            <th>OCRBench</th>
                            <th>MMVet</th>
                        </tr>
                        <tr>
                            <td>InternVL2-5-1B</td>
                            <td>54.9</td>
                            <td>66.5</td>
                            <td>51.3</td>
                            <td>41.2</td>
                            <td>47.1</td>
                            <td>39.4</td>
                            <td>69.0</td>
                            <td>774</td>
                            <td>47.2</td>
                        </tr>
                        <tr>
                            <td>InternVL2-5-1B-MPO</td>
                            <td>56.4</td>
                            <td>67.2</td>
                            <td>49.7</td>
                            <td>40.8</td>
                            <td>53.0</td>
                            <td>40.0</td>
                            <td>69.4</td>
                            <td>836</td>
                            <td>47.2</td>
                        </tr>
                        <tr>
                            <td>InternVL2-5-2B</td>
                            <td>59.9</td>
                            <td>70.9</td>
                            <td>54.3</td>
                            <td>43.2</td>
                            <td>51.1</td>
                            <td>42.3</td>
                            <td>74.9</td>
                            <td>802</td>
                            <td>62.6</td>
                        </tr>
                        <tr>
                            <td>InternVL2-5-2B-MPO</td>
                            <td>62.0</td>
                            <td>71.6</td>
                            <td>55.0</td>
                            <td>45.0</td>
                            <td>56.4</td>
                            <td>43.0</td>
                            <td>75.3</td>
                            <td>842</td>
                            <td>65.4</td>
                        </tr>
                        <tr>
                            <td>InternVL2-5-4B</td>
                            <td>65.1</td>
                            <td>78.2</td>
                            <td>58.7</td>
                            <td>51.8</td>
                            <td>60.8</td>
                            <td>46.6</td>
                            <td>81.4</td>
                            <td>820</td>
                            <td>61.5</td>
                        </tr>
                        <tr>
                            <td>InternVL2-5-4B-MPO</td>
                            <td>67.6</td>
                            <td>78.6</td>
                            <td>60.2</td>
                            <td>51.6</td>
                            <td>65.3</td>
                            <td>47.8</td>
                            <td>82.0</td>
                            <td>880</td>
                            <td>67.1</td>
                        </tr>
                        <tr>
                            <td>InternVL2-5-8B</td>
                            <td>68.9</td>
                            <td>82.5</td>
                            <td>63.2</td>
                            <td>56.2</td>
                            <td>64.5</td>
                            <td>49.0</td>
                            <td>84.6</td>
                            <td>821</td>
                            <td>62.8</td>
                        </tr>
                        <tr>
                            <td>InternVL2-5-8B-MPO</td>
                            <td>70.4</td>
                            <td>82.4</td>
                            <td>65.7</td>
                            <td>54.9</td>
                            <td>68.9</td>
                            <td>51.4</td>
                            <td>84.5</td>
                            <td>883</td>
                            <td>66.9</td>
                        </tr>
                        <tr>
                            <td>InternVL2-5-26B</td>
                            <td>71.6</td>
                            <td>84.6</td>
                            <td>66.5</td>
                            <td>60.7</td>
                            <td>68.0</td>
                            <td>55.8</td>
                            <td>86.2</td>
                            <td>854</td>
                            <td>65.4</td>
                        </tr>
                        <tr>
                            <td>InternVL2-5-26B-MPO</td>
                            <td>72.9</td>
                            <td>84.2</td>
                            <td>67.9</td>
                            <td>57.3</td>
                            <td>72.2</td>
                            <td>55.4</td>
                            <td>86.7</td>
                            <td>907</td>
                            <td>68.8</td>
                        </tr>
                        <tr>
                            <td>InternVL2-5-38B</td>
                            <td>73.5</td>
                            <td>85.4</td>
                            <td>68.5</td>
                            <td>64.6</td>
                            <td>72.4</td>
                            <td>57.9</td>
                            <td>87.6</td>
                            <td>841</td>
                            <td>67.2</td>
                        </tr>
                        <tr>
                            <td>InternVL2-5-38B-MPO</td>
                            <td>75.5</td>
                            <td>85.6</td>
                            <td>69.8</td>
                            <td>64.1</td>
                            <td>73.8</td>
                            <td>61.5</td>
                            <td>88.1</td>
                            <td>885</td>
                            <td>72.5</td>
                        </tr>
                        <tr>
                            <td>InternVL2-5-78B</td>
                            <td>75.2</td>
                            <td>87.5</td>
                            <td>69.5</td>
                            <td>70.0</td>
                            <td>70.6</td>
                            <td>57.4</td>
                            <td>89.1</td>
                            <td>853</td>
                            <td>71.8</td>
                        </tr>
                        <tr>
                            <td>InternVL2-5-78B-MPO</td>
                            <td>77.2</td>
                            <td>87.8</td>
                            <td>71.9</td>
                            <td>68.7</td>
                            <td>76.5</td>
                            <td>58.9</td>
                            <td>89.3</td>
                            <td>907</td>
                            <td>73.6</td>
                        </tr>
                    </table>

                    <h2 class="title">Citation</h2>
                    <pre><code>
  @article{wang2024mpo,
    title={Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization},
    author={Wang, Weiyun and Chen, Zhe and Wang, Wenhai and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Zhu, Jinguo and Zhu, Xizhou and Lu, Lewei and Qiao, Yu and Dai, Jifeng},
    journal={arXiv preprint arXiv:2411.10442},
    year={2024}
    }

  @article{chen2024expanding,
    title={Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling},
    author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},
    journal={arXiv preprint arXiv:2412.05271},
    year={2024}
  }

  @article{chen2024far,
    title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},
    author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
    journal={arXiv preprint arXiv:2404.16821},
    year={2024}
  }

  @inproceedings{chen2024internvl,
    title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
    author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={24185--24198},
    year={2024}
  }

</code></pre>
                    <br>
                    <h4 class="title"><a href="../">🔙 Go Back</a></h4>
                </div>


            </article>
        </div>
    </main>
    <footer class="site-footer h-card">
        <data class="u-url" href="/blog/"></data>
        <div class="wrapper">
            <div class="footer-col-wrapper">
                <div class="footer-col footer-col-1">
                    <ul class="contact-list">
                    </ul>
                </div>
                <div class="footer-col footer-col-2">
                    <ul class="social-media-list"></ul>
                </div>
                <div class="footer-col footer-col-3">
                    <p></p>
                </div>
            </div>
        </div>
    </footer>

    <!-- 轮播图 -->
    <script>
        let currentIndex = 0; // 当前显示的图片索引
        const slides = document.querySelectorAll('#carousel > ul > li'); // 获取所有图片
        const totalSlides = slides.length; // 图片总数

        // 设置ul的宽度为总图片数量的百分比
        document.querySelector('#carousel > ul').style.width = (totalSlides * 100) + '%';

        function moveSlide(direction) {
            currentIndex += direction; // 根据方向增加或减少索引
            if (currentIndex < 0) {
                currentIndex = totalSlides - 1; // 如果小于0，循环到最后一张
            } else if (currentIndex >= totalSlides) {
                currentIndex = 0; // 如果超过总数，循环到第一张
            }
            updateCarousel(); // 更新轮播图显示
        }

        function updateCarousel() {
            const slideWidth = document.querySelector('#carousel').offsetWidth; // 获取轮播图宽度
            const offset = -currentIndex * slideWidth; // 每张图片的宽度相对于容器宽度
            document.querySelector('#carousel > ul').style.left = offset + 'px'; // 更新位置
        }
    </script>

    <!-- 数学公式 -->
    <script>
        elements = document.getElementsByClassName("MMPR-Positive-Set")
        for (let element of elements) {
            katex.render("\\mathcal{Y}_p", element);
        }

        elements = document.getElementsByClassName("MMPR-Negative-Set")
        for (let element of elements) {
            katex.render("\\mathcal{Y}_n", element);
        }

        katex.render("\\mathcal{L}_{\\text{p}}", document.getElementById("pref-loss-left"));
        katex.render("\\mathcal{L}_{\\text{q}}", document.getElementById("quality-loss-left"));
        katex.render("\\mathcal{L}_{\\text{q}}^{+}", document.getElementById("quality-positive-loss-left"));
        katex.render("\\mathcal{L}_{\\text{q}}^{-}", document.getElementById("quality-negative-loss-left"));
        katex.render("\\mathcal{L}_{\\text{g}}", document.getElementById("gen-loss-left"));
        katex.render("x", document.getElementById("x"));
        katex.render("w_*", document.getElementById("w_i"));
        katex.render("y_c", document.getElementById("y_c"));
        katex.render("y_r", document.getElementById("y_r"));
        katex.render("\\pi_0", document.getElementById("pi_0"));
        katex.render("\\pi_\\theta", document.getElementById("pi_theta"));
        katex.render("\\delta", document.getElementById("delta"));
        katex.render("\\beta", document.getElementById("beta"));

        katex.render("\\mathcal{L}=w_{p}\\cdot\\mathcal{ L }_{ \\text{ p }} + w_{q}\\cdot\\mathcal{ L }_{ \\text{ q }} + w_{g}\\cdot\\mathcal{ L }_{ \\text{ g }}, ", document.getElementById("overall-loss"));
        katex.render("\\mathcal{L}_{\\text{p}}=-\\log \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta\\left(y_c \\mid x\\right)}{\\pi_0\\left(y_c \\mid x\\right)}-\\beta \\log \\frac{\\pi_\\theta\\left(y_r \\mid x\\right)}{\\pi_0\\left(y_r \\mid x\\right)}\\right),", document.getElementById("pref-loss"));
        katex.render("\\mathcal{L}_{\\text{q}}=\\mathcal{L}_{\\text{q}}^+ + \\mathcal{L}_{\\text{q}}^-,", document.getElementById("quality-loss"));
        katex.render("\\mathcal{L}_{\\text{q}}^+=-\\log \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta\\left(y_c \\mid x\\right)}{\\pi_0\\left(y_c \\mid x\\right)} - \\delta\\right),", document.getElementById("quality-positive-loss"));
        katex.render("\\mathcal{L}_{\\text{q}}^-=-\\log \\sigma\\left(-\\left(\\beta \\log \\frac{\\pi_\\theta\\left(y_r \\mid x\\right)}{\\pi_0\\left(y_r \\mid x\\right)} - \\delta\\right) \\right),", document.getElementById("quality-negative-loss"));
        katex.render("\\mathcal{L}_{\\text{g}}=-\\frac{\\log\\pi_\\theta\\left(y_c \\mid x\\right)}{\\left| y_c \\right|}.", document.getElementById("gen-loss"));
    </script>

</body>

</html>