<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/image/your_banner_image.png" /> -->
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>SDLM</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Sequential Diffusion Language Models</h1>
            <!-- <h5 class="subtitle is-4 publication-awards">CVPR 2025</h5> -->
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=en&user=_q9pCk4AAAAJ"
                  style="color:#f68946;font-weight:normal;">Yangzhou Liu*</a>,
                <a href="https://scholar.google.com/citations?hl=en&user=eDa196cAAAAJ"
                  style="color:#f68946;font-weight:normal;">Yue Cao*</a>,
                <a href="https://scholar.google.com/citations?hl=en&user=qHqQsY4AAAAJ"
                  style="color:#f68946;font-weight:normal;">Hao Li*</a>,
                <a href="https://scholar.google.com/citations?hl=en&user=EyZqU9gAAAAJ"
                  style="color:#f68946;font-weight:normal;">Gen Luo*</a>,
                <a href="https://scholar.google.com/citations?hl=en&user=j1rq_lYAAAAJ"
                  style="color:#f68946;font-weight:normal;">Zhe Chen</a>,    
                <a href="https://scholar.google.com/citations?hl=en&user=MRTB_-wAAAAJ"
                  style="color:#f68946;font-weight:normal;">Weiyun Wang</a>,  
                <a href="https://scholar.google.com/citations?hl=en&user=hF4-7akAAAAJ"
                  style="color:#f68946;font-weight:normal;">Xiaobo Liang</a>,
                <br>
                  <a href="https://biqing-qi.github.io/"
                  style="color:#f68946;font-weight:normal;">Biqing Qi</a>,
                <a href="https://scholar.google.com/citations?hl=en&user=RD5kSG0AAAAJ"
                  style="color:#f68946;font-weight:normal;">Lijun Wu</a>,
                <a href="https://scholar.google.com/citations?hl=en&user=kQ3AisQAAAAJ"
                  style="color:#f68946;font-weight:normal;">Changyao Tian</a>,
                <a href="https://zhangyanting.github.io/"
                  style="color:#f68946;font-weight:normal;">Yanting Zhang</a>,
                <a href="https://scholar.google.com/citations?hl=en&user=RQqws5gAAAAJ"
                  style="color:#f68946;font-weight:normal;">Yuqiang Li</a>,
                <br>
                  <a href="https://scholar.google.com/citations?hl=en&user=mgqhQGkAAAAJ"
                  style="color:#f68946;font-weight:normal;">Tong Lu</a>,
                <a href="https://scholar.google.com/citations?hl=en&user=gFtI-8QAAAAJ"
                  style="color:#f68946;font-weight:normal;">Yu Qiao</a>,
                <a href="https://scholar.google.com/citations?hl=en&user=SH_-B_AAAAAJ"
                  style="color:#f68946;font-weight:normal;">Jifeng Dai</a>,
                <a href="https://scholar.google.com/citations?hl=en&user=WM0OglcAAAAJ"
                  style="color:#f68946;font-weight:normal;">Wenhai Wang</a>
              </span>

              <div class="is-size-5 publication-authors">
                <span class="author-block"><b style="color:#008AD7; font-weight:normal">â–¶ </b> Shanghai AI
                  Laboratory</span>
                <span class="author-block"><b style="color:#bb52ff; font-weight:normal">â–¶ </b> Nanjing University</span>
                <span class="author-block"><b style="color:#F2A900; font-weight:normal">â–¶ </b> Tsinghua
                  University</span>
                <br>
                <span class="author-block"><b style="color:#f68946; font-weight:normal">â–¶ </b> Fudan University</span>
                <span class="author-block"><b style="color:#00f2c2; font-weight:normal">â–¶ </b> The Chinese University of Hong Kong</span>
                <br>
                <span class="author-block"><b style="color:#ff481c; font-weight:normal">â–¶ </b> Soochow University</span>
                <span class="author-block"><b style="color:#ff481c; font-weight:normal">â–¶ </b> Donghua University</span>
                <div class="is-size-6 publication-authors">
                  <span class="author-block"><b>*</b> Equal contribution.</span>
                </div>
              </div>

              <div class="column has-text-centered">

                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://huggingface.co/papers/2509.24007" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" style="color:#ffffff">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span style="color:#ffffff">arXiv</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://github.com/OpenGVLab/SDLM" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" style="color:#ffffff">
                        <i class="fab fa-github"></i>
                      </span>
                      <span style="color:#ffffff">Code</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://huggingface.co/collections/OpenGVLab/sdlm-68ac82709d7c343ad36aa552" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" style="color:#ffffff">
                        ðŸ¤—
                      </span>
                      <span style="color:#ffffff">Model</span>
                    </a>
                  </span>

                  <!-- <i class="fas fa-download"></i> -->
                  <!-- <span class="link-block">
                  <a href="https://internvl.github.io/blog/2025-05-26-VeBrain/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon" style="color:#ffffff">
                      <i class="fas fa-edit"></i>
                    </span>
                    <span style="color:#ffffff">Chinese Post</span>
                  </a>
                </span> -->
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
  </section>

  
  <!-- Paper abstract -->
  <section class="section" style="background-color:#ffffffff">
    <div class="container is-max-desktop">

      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <!-- Paper video. -->
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <!-- <h2 class="title is-3"></h2> -->

              <video id="dollyzoom" autoplay controls muted playsinline height="100%">
                <source src="static/videos/case_long.mp4" type="video/mp4">
              </video>
            </div>
          </div>
          <!-- <br> -->
          <br>
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Diffusion language models (DLMs) have strong theoretical efficiency but are limited by fixed-length decoding and incompatibility with key-value (KV) caches. Block diffusion mitigates these issues, yet still enforces a fixed block size and requires expensive training. We introduce Next Sequence Prediction (NSP), which unifies next-token and next-block prediction, enabling the model to adaptively determine the generation length at each step. When the length is fixed to 1, NSP reduces to standard next-token prediction. Building on NSP, we propose <b>S</b>equential <b>D</b>iffusion <b>L</b>anguage <b>M</b>odel (<b>SDLM</b>), which can retrofit pre-trained autoregressive language models (ALMs) at minimal cost. Specifically, SDLM performs diffusion inference within fixed-size mask blocks, but dynamically decodes consecutive subsequences based on model confidence, thereby preserving KV-cache compatibility and improving robustness to varying uncertainty and semantics across the sequence. Experiments show that SDLM matches or surpasses strong autoregressive baselines using only <b>3.5M training samples</b>, while achieving <b>2.1Ã— higher throughput</b> than Qwen-2.5. Notably, the SDLM-32B model delivers even more pronounced efficiency gains, demonstrating the strong scalability potential of our modeling paradigm.
            </p>

            <div style="text-align: center;">
              <img id="mono_internvl" width="90%" src="static/images/framwork_compare.png">
            </div>

          </div>
        </div>
      </div>

    </div>
  </section>
  <!-- End paper abstract -->

  <section class="section" style="background-color:#efeff081">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Sequential Diffusion Language Model</h2>
      </div>
    </div>
    <!-- </div> -->
    <!--/ Results. -->
    <div class="container is-max-desktop">

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified">
            <p>
              We propose a sequential blockwise masked prediction method that reduces error accumulation in diffusion-based generation. Our method leverages the observation that predictions for tokens at lower positional indices typically benefit from more reliable contextual information, resulting in lower deviation and improved accuracy.

            <ul type="1">
              <li><b>(a) Training pipeline</b>. <span style="font-size: 95%;">Reordered input enables structured mask with causal prefix (top-left), visible cross-block prefix (bottom-left), and intra-block bidirectional attention (bottom-right). </span></li>
              <li><b>(b) Sampling Pipeline</b>. <span style="font-size: 95%;">Confidence-based dynamic block decoding with KV cache reuse. At each step, a block of D tokens is predicted with D - 1 padding masks. The longest high-confidence prefix is selected as dynamic output. Cached KV states enable efficient decoding. </span></li>
              <br>
            </ul>

            <centering>
              <div style="text-align: center;">
                <img id="pipeline" width="90%" src="static/images/framework.png">
              </div>
            </centering>
            
            </p>
          </div>

        </div>
      </div>
    </div>

  </section>

  <!-- Exp. -->
  <section class="section" style="background-color:#ffffffff">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Performance</h2>
      </div>
    </div>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-4">Long-Form Benchmarks</h2>
          <p>
            SDLM delivers strong performance with significantly faster decoding speed. It operates approximately 2x faster than comparable autoregressive models while matching their accuracy, and achieves up to 5x speedup over other diffusion language models, as evidenced by results on the MATH-500 benchmark.
          </p>
          <div style="text-align: center;">
            <img id="ablation" width="80%" src="static/images/main_exp1.png">
          </div>
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-4">General Mutiple-Choice Benchmarks</h2>
          <div style="text-align: center;">
            <img id="multimodal" width="55%" src="static/images/main_exp2.png">
          </div>
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-4">Block Size & Self-Speculative Decoding</h2>
          <div style="text-align: center;">
            <img id="multimodal" width="70%" src="static/images/self_speculative_decoding.png">
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Stade off. -->
  <section class="section" style="background-color:#efeff081">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Trade-off Between Performance and Speed</h2>
      </div>
    </div>

    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified">
            <p>
              Trade-off between performance and speed under different confidence thresholds Ï„ for SDLM-3B (D=4) and SDLM-3B (D=8). By adjusting Ï„, a controllable trade-off between speed and performance can be achieved. SpeedUp denotes the average number of tokens output per forward pass.

            <centering>
              <div style="text-align: center;">
                <img id="pipeline" width="85%" src="static/images/ablation_tau.png">
              </div>
            </centering>

            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
    <pre>
<code>@article{liu2025sdlm,
  title={Sequential Diffusion Language Models},
  author={Liu, Yangzhou and Cao, Yue and Li, Hao and Luo, Gen and Chen, Zhe and Wang, Weiyun and Liang, Xiaobo and Qi, Biqing and Wu, Lijun and Tian, Changyao and Zhang, Yanting and Li, Yuqiang and Lu, Tong and Qiao, Yu and Dai, Jifeng and Wang, Wenhai},
  journal={arXiv preprint arXiv:2509.24007},
  year={2025}
}
</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the
              footer. <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>