<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>InternVL 1.5: How Far Are We to GPT-4V?</title>
    <meta name="description" content="InternVL 1.5 achieves performance parity with top commercial models like GPT-4V, demonstrating the power of open-source vision-language models.">
    <link rel="stylesheet" href="/blog/assets/main.css">
</head>
<body>
    <header class="site-header" role="banner">
        <div class="wrapper">
            <a class="site-title" rel="author" href="/blog/">InternVL</a>
            <nav class="site-nav">
                <input type="checkbox" id="nav-trigger" class="nav-trigger"/>
                <label for="nav-trigger">
                    <span class="menu-icon">
                        <svg viewBox="0 0 18 15" width="18px" height="15px">
                            <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
                        </svg>
                    </span>
                </label>
                <div class="trigger"></div>
            </nav>
        </div>
    </header>
    <main class="page-content" aria-label="Content">
        <div class="wrapper">
            <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
                <header class="post-header">
                    <h1 class="post-title p-name" itemprop="name headline">Open-source vision-language model now comparable to GPT-4V.</h1>
                    <p class="post-meta">
                        <time class="dt-published" datetime="2024-05-20" itemprop="datePublished">May 20, 2024</time>
                        â€¢ 
                        <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                            <span class="p-author h-card" itemprop="name">InternVL Team</span>
                        </span>
                    </p>
                </header>
                <div class="post-content e-content" itemprop="articleBody">
                    <p>Research and development in multimodal large models are advancing rapidly, but despite significant progress, there has always been a gap in capabilities between open-source and commercial models. The OpenGVLab team from Shanghai AI Laboratory, in partnership with Tsinghua University and SenseTime, recently released a new open-source multimodal large language model project called InternVL 1.5. This project aims to challenge the dominance of commercial model giants like GPT-4V and raises the question of how far the power of open source can go.</p>
                    <p>Today, InternVL has been the top 1 Huggingface trending model for both image feature extraction and VQA.</p>
                    <img src="images/1_5_1.png" alt="InternVL Trending Model">
                    <img src="images/1_5_2.png" alt="InternVL Trending Model">

                    <p>As the latest generation of the open-source vision-language model (VLM) or multimodal large language models (MLLM), InternVL 1.5 achieves performance parity with top commercial models like GPT-4V and demonstrates outstanding technical advantages in the open-source domain. The power of the open-source community is once again affirmed, driving not only technological advancement but also the construction of a tech ecosystem that emphasizes sharing, cooperation, and innovation.</p>
                    <img src="images/1_5_3.png" alt="InternVL Performance">

                    <h2>Technical Report:</h2>
                    <a href="https://arxiv.org/pdf/2404.16821">https://arxiv.org/pdf/2404.16821</a>
                    <img src="images/1_5_4.png" alt="Technical Report">

                    <h2>Demo (Try it!)</h2>
                    <a href="https://huggingface.co/spaces/OpenGVLab/InternVL">https://huggingface.co/spaces/OpenGVLab/InternVL</a>

                    <h2>Project Code:</h2>
                    <a href="https://github.com/OpenGVLab/InternVL">https://github.com/OpenGVLab/InternVL</a>

                    <h2>3 Highlights of InternVL 1.5</h2>
                    <h3>Powerful Visual Encoder</h3>
                    <p>InternVL 1.5, through its unique visual encoder, InternViT-6B, employs a continuous learning strategy to greatly enhance the depth and breadth of visual understanding. This strategy enables InternViT-6B to achieve seamless transfer and reuse among various large language models, strengthening the model's ability to parse complex visual content and exhibit more precise recognition and interpretation capabilities in image-intensive tasks.</p>

                    <h3>Dynamic High Resolution</h3>
                    <p>InternVL 1.5 introduces a brand-new dynamic high-resolution strategy for image processing. This feature supports input resolutions of up to 4K, optimizing the presentation of image details and improving the model's expressiveness and accuracy on high-resolution images, while also ensuring efficient computation. This revolutionary feature significantly enhances the overall image processing performance and is expected to be a game-changer in the field of image processing.</p>

                    <h3>High-Quality Bilingual Dataset</h3>
                    <p>InternVL 1.5 integrates a wide range of high-quality bilingual datasets covering English and Chinese, significantly improving the model's operational flexibility and accuracy in multilingual environments. In addition, through the data translation pipeline developed by open-source large language models, InternVL 1.5 can automatically expand to more languages, showing tremendous potential in global applications.</p>

                    <h2>Evaluation of InternVL 1.5 VS GPT4V</h2>
                    <h3>Benchmark Evaluation</h3>
                    <p>To evaluate InternVL 1.5's performance, the research team conducted extensive evaluations in 18 multimodal benchmark tests. These benchmarks cover various aspects, including OCR-related, general multimodal, mathematical, and multi-turn dialogues. InternVL 1.5 performs well on multiple key dimensions, narrowing the gap between open-source models and GPT-4V, especially reaching the SOTA level in tasks such as OCR, MMB, SEED, and Math.</p>
                    <img src="images/1_5_5.png" alt="Benchmark Evaluation">

                    <h3>Evaluation on Real Case</h3>
                    <h4>General QA</h4>
                    <p>InternVL 1.5 was tested with everyday user questions to evaluate its performance in general question answering. In the first example, InternVL 1.5 demonstrated a good understanding of the image and correctly explained the movement in the image. It also distinguished the positioning of two individuals, slightly outperforming GPT-4V in detail. In the second example, InternVL 1.5 identified the action as imitating Bruce Lee's posture, while GPT-4V failed to answer this question.</p>
                    <img src="images/1_5_6.png" alt="General QA">

                    <h4>OCR-related QA</h4>
                    <p>Compare the OCR ability of the InternVL 1.5 and GPT-4V with two small cases. In the first test, attention was paid to the model's understanding of Chinese scenes. The results show that GPT-4V performs poorly in extracting all useful information from the image, while InternVL 1.5 can more accurately identify and parse details in the image. The second test focuses on graph understanding. GPT-4V and InternVL 1.5 can effectively parse graph data and structure, identifying the highest and lowest years, but GPT-4V fails to answer the final difference accurately. In contrast, InternVL 1.5 accurately answers the difference as "1580 billion RMB".</p>
                    <img src="images/1_5_7.png" alt="OCR-related QA">

                    <h4>Scientific Understanding QA</h4>
                    <p>In the field of multimodal large language models, models perform poorly in complex scenarios involving domain-specific knowledge and logical reasoning. However, in the first question shown in the figure below, the InternVL 1.5 model not only accurately analyzes the elements in the image but also provides precise answers, while GPT-4V demonstrates its unique insights in inferring trends in amino acid transport. In the second example, both models can accurately answer and provide in-depth analysis from the perspective of aerodynamics, demonstrating their efficient capabilities in handling scientific problems. These achievements also demonstrate the comparability of InternVL 1.5 and GPT-4V in scientific understanding and reasoning abilities.</p>
                    <img src="images/1_5_8.png" alt="Scientific Understanding QA">

                    <h4>Object Localization</h4>
                    <p>Accurate object localization techniques become crucial in multimodal tasks. In the following tests, the InternVL 1.5 model performs exceptionally well. It not only quickly and accurately identifies the specific position of the basketball player but also describes the posture of the corresponding player in detail. It can also accurately interpret dynamic interactions in complex scenes, such as correctly identifying the man's right hand pointing and understanding the meaning the man wants to convey based on the actual scene.</p>
                    <img src="images/1_5_9.png" alt="Object Localization">

                    <h2>Conclusion</h2>
                    <p>InternVL 1.5 is not just a project but also an important contribution of the open-source community to high-level multimodal artificial intelligence technology. In the commercial field, multimodal models like GPT-4V have become industry benchmarks. In this context, InternVL 1.5 demonstrates that the open-source community can not only keep pace with the commercial field but also lead in some aspects.</p>

                    <p>InternVL 1.5 significantly improves the performance of open-source multimodal models and narrows the gap with commercial models. This open-source model is expected to drive the development of the multimodal field and deserves further exploration and optimization.</p>

                    <p>As a completely open-source project, InternVL 1.5 encourages everyone interested to participate and jointly promote the future development of AI. By visiting the online demo of InternVL 1.5 (<a href="https://internvl.opengvlab.com">https://internvl.opengvlab.com</a>), researchers and developers worldwide can now experience the charm of this technology and participate in this exciting open-source project.</p>
                </div>
                <a class="u-url" href="/blog/2024-05-20-InternVL-1-5-vs-GPT-4V" hidden></a>
            </article>
        </div>
    </main>
    <footer class="site-footer h-card">
        <data class="u-url" href="/blog/"></data>
        <div class="wrapper">
            <h2 class="footer-heading">InternVL</h2>
            <div class="footer-col-wrapper">
                <div class="footer-col footer-col-1">
                    <ul class="contact-list">
                        <li class="p-name">InternVL</li>
                    </ul>
                </div>
                <div class="footer-col footer-col-2">
                    <ul class="social-media-list"></ul>
                </div>
                <div class="footer-col footer-col-3">
                    <p></p>
                </div>
            </div>
        </div>
    </footer>
</body>
</html>
