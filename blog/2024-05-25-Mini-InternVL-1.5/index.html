<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <!-- Begin Jekyll SEO tag v2.8.0 -->
        <title>Mini-InternVL 1.5: A Powerful Pocket Multimodal Model for Edge Devices</title>
        <meta name="generator" content="Jekyll v3.9.4"/>
        <meta name="author" content="chenzhe"/>
        <meta property="og:locale" content="en_US"/>
        <meta property="og:site_name" content="InternVL"/>
        <meta property="og:type" content="article"/>
        <meta property="article:published_time" content="2024-01-30T12:33:38-06:00"/>
        <meta name="twitter:card" content="summary"/>
        <!-- End Jekyll SEO tag -->
    <link rel="icon" href="https://github-production-user-asset-6210df.s3.amazonaws.com/47669167/330728723-7037290e-f474-4d11-b90f-1d8316087bf8.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240529%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240529T072300Z&X-Amz-Expires=300&X-Amz-Signature=d12b9e5c3c49a082747f5da55529a4f1247cd17b4329fafc1cb6d1c0678efa77&X-Amz-SignedHeaders=host&actor_id=23737120&key_id=0&repo_id=721995615">
        <link rel="stylesheet" href="/blog/assets/main.css">
        <link type="application/atom+xml" rel="alternate" href="https://llava-vl.github.io/blog/feed.xml" title="LLaVA"/>
    </head>
    <body>
        <header class="site-header" role="banner">
            <div class="wrapper">
                <a class="site-title" rel="author" href="/blog/">InternVL</a>
                <nav class="site-nav">
                    <input type="checkbox" id="nav-trigger" class="nav-trigger"/>
                    <label for="nav-trigger">
                        <span class="menu-icon">
                            <svg viewBox="0 0 18 15" width="18px" height="15px">
                                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
                            </svg>
                        </span>
                    </label>
                    <div class="trigger"></div>
                </nav>
            </div>
        </header>
        <main class="page-content" aria-label="Content">
            <div class="wrapper">
                <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
                    <header class="post-header">
                        <h1 class="post-title p-name" itemprop="name headline">Mini-InternVL 1.5: A Powerful Pocket Multimodal Model for Edge Devices</h1>
                        <p class="post-meta">
                            <time class="dt-published" datetime="2024-05-20" itemprop="datePublished">2024/05/25</time>
                            • 
                            <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                                <span class="p-author h-card" itemprop="name">Zhe Chen*, Zhangwei Gao*, Erfei Cui*, Wenhai Wang*, Weiyun Wang, Jinguo Zhu, Hao Tian, Xizhou Zhu, Lewei Lu, Tong Lu, Yu Qiao, Jifeng Dai (* Equal Contribution)</span>
                            </span>
                        </p>
                    </header>
                    <p><strong><span style="color: red;">8% of the model size, 80% of the performance; 16% of the model size, 90% of the performance.</span></strong></p>

                    <table>
                        <thead>
                        <tr>
                        <th>Type</th>
                        <th>Model</th>
                        <th>Date</th>
                        <th>Download</th>
                        <th>Note</th>
                        </tr>
                        </thead>
                        <tbody>

                        <tr>
                        <td rowspan="2">Vision Large Language Model</td>
                        <td>Mini-InternVL-Chat-2B-V1-5</td>
                        <td>2024.05.19</td>
                        <td>🤗 <a href="https://huggingface.co/OpenGVLab/Mini-InternVL-Chat-2B-V1-5" rel="nofollow">HF link</a></td>
                        <td>🚀🚀 Only 2B parameters, anyone can deploy it locally.</td>
                        </tr>
                        <tr>
                        <td>Mini-InternVL-Chat-4B-V1-5</td>
                        <td>2024.05.28</td>
                        <td>🤗 <a href="https://huggingface.co/OpenGVLab/Mini-InternVL-Chat-4B-V1-5" rel="nofollow">HF link</a></td>
                        <td>🚀🚀 Only 4B parameters, anyone can deploy it locally.</td>
                        </tr>
                        <tr>
                         <td>Vision Foundation Model</td>
                            <td>InternViT−300M−448px</td>
                        <td>2024.05.25</td>
                        <td>🤗 <a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px" rel="nofollow">HF link</a></td>
                        <td>Distilled small vision foundation model with 300M parameters.</td>
                        </tr>

                        </tbody>
                    </table>
                    <div class="post-content e-content" itemprop="articleBody">
                        <p>You can run multimodal large models using a 1080Ti now.</p>
                        <p>We are delighted to introduce Mini-InternVL-Chat series. In the era of large language models, many researchers have started to focus on smaller language models, such as Gemma-2B, Qwen-1.8B, and InternLM2-1.8B. Inspired by their efforts, we have distilled our vision foundation model <a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5">InternViT-6B-448px-V1-5</a> down to 300M and used <a href="https://huggingface.co/internlm/internlm2-chat-1_8b">InternLM2-Chat-1.8B</a> or <a href="https://huggingface.co/microsoft/Phi-3-mini-128k-instruct">Phi-3-mini-128k-instruct</a> as our language model. This resulted in a small multimodal model with excellent performance.</p>

                        <p>As shown in the figure below, we adopted the same model architecture as InternVL 1.5. We simply replaced the original InternViT-6B with InternViT-300M and InternLM2-Chat-20B with InternLM2-Chat-1.8B or Phi-3-mini-128k-instruct. For training, we used the same data as InternVL 1.5 to train this smaller model. Additionally, due to the lower training costs of smaller models, <b>we used a context length of 8K during training.</b></p>
                        <center>
                        <img width="80%" alt="image" src="images/mini_internvl_architecture.png"><br>
                        </center>
                        <p>From the experimental results, we've observed that our distilled small vision model (InternViT-300M) is well-suited for a smaller language model (1.8B or 3.8B). This combination maximizes efficiency while maintaining impressive performance across various benchmarks, demonstrating the effectiveness of small models in handling complex tasks. Additionally, our small model significantly reduces memory requirements, making it more accessible and efficient for practical use.</p>

                        <h3>Performance</h3>

                        <table class="performance">

                            <thead>
                                <tr>
                                    <th>model</th>
                                    <th>open-<br>source</th>
                                    <th>#param</th>
                                    <th>DocVQA<br>(test)</th>
                                    <th>ChartQA<br>(test)</th>
                                    <th>InfoVQA<br>(test)</th>
                                    <th>TextVQA<br>(val)</th>
                                    <th>OCR<br>Bench</th>
                                    <th>MME</th>
                                    <th>RWQA</th>
                                    <th>AI2D<br>(test)</th>
                                    <th>MMMU<br>(val)</th>
                                    <th>MMB−EN/CN<br>(test)</th>
                                    <th>CCBench<br>(dev)</th>
                                    <th>MMVet</th>
                                    <th>SEED<br>(image)</th>
                                    <th>HallB</th>
                                    <th>MathVista<br>(mini)</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr style="background-color: rgba(117, 209, 215, 0.1);">
                                    <td>GPT-4V</td>
                                    <td>✗</td>
                                    <td>-</td>
                                    <td>88.4</td>
                                    <td>78.5</td>
                                    <td>-</td>
                                    <td>78.0</td>
                                    <td>645</td>
                                    <td>1926.6</td>
                                    <td>61.4</td>
                                    <td>78.2</td>
                                    <td>56.8</td>
                                    <td>77.0 / 74.4</td>
                                    <td>46.5</td>
                                    <td>67.6</td>
                                    <td>71.6</td>
                                    <td>46.5</td>
                                    <td>49.9</td>
                                </tr>

                                <tr style="background-color: rgba(117, 209, 215, 0.1);">
                                    <td>Gemini Pro 1.0</td>
                                    <td>✗</td>
                                    <td>-</td>
                                    <td>88.1</td>
                                    <td>74.1</td>
                                    <td>75.2</td>
                                    <td>74.6</td>
                                    <td>659</td>
                                    <td>1933.4</td>
                                    <td>-</td>
                                    <td>73.9</td>
                                    <td>47.9</td>
                                    <td>73.6 / 74.3</td>
                                    <td>52.5</td>
                                    <td>64.3</td>
                                    <td>70.7</td>
                                    <td>45.2</td>
                                    <td>45.2</td>
                                </tr>
                                <tr style="background-color: rgba(117, 209, 215, 0.1);">
                                    <td>Gemini Pro 1.5</td>
                                    <td>✗</td>
                                    <td>-</td>
                                    <td>86.5</td>
                                    <td>81.3</td>
                                    <td>72.7</td>
                                    <td>73.5</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>67.5</td>
                                    <td>80.3</td>
                                    <td>58.5</td>
                                    <td>- / -</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>52.1</td>
                                </tr>
                                <tr style="background-color: rgba(117, 209, 215, 0.1);">
                                    <td>Qwen-VL-Plus</td>
                                    <td>✗</td>
                                    <td>-</td>
                                    <td>91.4</td>
                                    <td>78.1</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>694</td>
                                    <td>2183.4</td>
                                    <td>-</td>
                                    <td>75.9</td>
                                    <td>45.2</td>
                                    <td>67.0 / 70.7</td>
                                    <td>55.1</td>
                                    <td>61.1</td>
                                    <td>72.7</td>
                                    <td>40.6</td>
                                    <td>43.3</td>
                                </tr>
                                <tr style="background-color: rgba(117, 209, 215, 0.1);">
                                    <td>Claude-3 Haiku</td>
                                    <td>✗</td>
                                    <td>-</td>
                                    <td>88.8</td>
                                    <td>81.7</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>658</td>
                                    <td>1453.2</td>
                                    <td>-</td>
                                    <td>86.7</td>
                                    <td>50.2</td>
                                    <td>60.7 / 57.2</td>
                                    <td>24.5</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>39.2</td>
                                    <td>46.4</td>
                                </tr>

                                <tr style="background-color: rgba(117, 209, 215, 0.1);">
                                    <td>Step-1V</td>
                                    <td>✗</td>
                                    <td>100B</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>625</td>
                                    <td>2206.4</td>
                                    <td>-</td>
                                    <td>79.2</td>
                                    <td>49.9</td>
                                    <td>80.7 / 79.9</td>
                                    <td>71.2</td>
                                    <td>63.3</td>
                                    <td>70.3</td>
                                    <td>48.4</td>
                                    <td>44.8</td>
                                </tr>
                                <tr style="background-color: rgba(117, 209, 215, 0.1);">
                                    <td>Grok-1.5V</td>
                                    <td>✗</td>
                                    <td>-</td>
                                    <td>85.6</td>
                                    <td>76.1</td>
                                    <td>-</td>
                                    <td>78.1</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>68.7</td>
                                    <td>88.3</td>
                                    <td>-</td>
                                    <td>- / -</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>52.8</td>
                                </tr>
                                <tr style="background-color: rgba(249, 242, 248, 1);">
                                    <td>LLaVA-NeXT−34B</td>
                                    <td>✓</td>
                                    <td>35B</td>
                                    <td>84.0</td>
                                    <td>68.7</td>
                                    <td>51.5</td>
                                    <td>69.5</td>
                                    <td>574</td>
                                    <td>2028.0</td>
                                    <td>-</td>
                                    <td>74.9</td>
                                    <td>51.1</td>
                                    <td>81.1 / 79.0</td>
                                    <td>49.2</td>
                                    <td>57.4</td>
                                    <td>75.9</td>
                                    <td>34.8</td>
                                    <td>46.5</td>
                                </tr>
                                <tr style="background-color: rgba(249, 242, 248, 1);">
                                    <td>LLaVA-NeXT−110B</td>
                                    <td>✓</td>
                                    <td>112B</td>
                                    <td>85.7<sup>(val)</sup></td>
                                    <td>79.7</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>2200.4</td>
                                    <td>63.1</td>
                                    <td>80.4</td>
                                    <td>49.1</td>
                                    <td>- / -</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>49.0</td>
                                </tr>
                                <tr style="background-color: rgba(249, 242, 248, 1);">
                                    <td>InternVL 1.2</td>
                                    <td>✓</td>
                                    <td>40B</td>
                                    <td>57.7</td>
                                    <td>68.0</td>
                                    <td>39.5</td>
                                    <td>72.5</td>
                                    <td>569</td>
                                    <td>2175.4</td>
                                    <td>67.5</td>
                                    <td>79.0</td>
                                    <td>51.6</td>
                                    <td>82.2 / 81.2</td>
                                    <td>59.2</td>
                                    <td>48.9</td>
                                    <td>75.6</td>
                                    <td>47.6</td>
                                    <td>47.7</td>
                                </tr>
                                <tr style="background-color: rgba(249, 242, 248, 1);">
                                    <td>InternVL 1.5</td>
                                    <td>✓</td>
                                    <td>25.5B</td>
                                    <td>90.9</td>
                                    <td>83.8</td>
                                    <td>72.5</td>
                                    <td>80.6</td>
                                    <td>724</td>
                                    <td>2187.8</td>
                                    <td>66.0</td>
                                    <td>80.7</td>
                                    <td>45.2</td>
                                    <td>82.2 / 82.0</td>
                                    <td>69.8</td>
                                    <td>62.8</td>
                                    <td>76.0</td>
                                    <td>49.3</td>
                                    <td>53.5</td>
                                </tr>

                                <tr style="background-color: rgb(244,252,255);">
                                    <td colspan="18"></td>
                                </tr>

                                <tr style="background-color: rgb(244,252,255);">
                                    <td>MobileVLM-V2-1.7B</td>
                                    <td>✓</td>
                                    <td>1.7B</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>52.1</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                </tr>
                                <tr style="background-color: rgb(244,252,255);">
                                    <td>MobileVLM-V2-3B</td>
                                    <td>✓</td>
                                    <td>3.0B</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>57.5</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                </tr>

                                </tr>
                                    <tr style="background-color: rgba(244,252,255, 1);">
                                    <td>Mini-Gemini-2B</td>
                                    <td>✓</td>
                                    <td>3.5B</td>
                                    <td>34.2</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>56.2</td>
                                    <td>-</td>
                                    <td>1653.0</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>31.7</td>
                                    <td>- / -</td>
                                    <td>-</td>
                                    <td>31.1</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>29.4</td>
                                </tr>

                                </tr>
                                    <tr style="background-color: rgba(244,252,255, 1);">
                                    <td>Bunny-v1.0-3B</td>
                                    <td>✓</td>
                                    <td>3.2B</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>1778.1</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>38.2</td>
                                    <td>69.2 / -</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                </tr>
                                </tr>
                                    <tr style="background-color: rgba(244,252,255, 1);">
                                    <td>Bunny-v1.1-4B</td>
                                    <td>✓</td>
                                    <td>4.3B</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>1866.8</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>40.2</td>
                                    <td>74.1 / 66.3</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>71.7</td>
                                    <td>-</td>
                                    <td>-</td>
                                </tr>
                                <tr style="background-color: rgb(244,252,255);">
                                    <td>DeepSeek−VL−1.3B</td>
                                    <td>✓</td>
                                    <td>2.0B</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>57.8</td>
                                    <td>409</td>
                                    <td>1531.6</td>
                                    <td>49.7</td>
                                    <td>51.5</td>
                                    <td>32.2</td>
                                    <td>66.4 / 62.9</td>
                                    <td>37.6</td>
                                    <td>34.8</td>
                                    <td>66.7</td>
                                    <td>27.6</td>
                                    <td>31.1</td>
                                </tr>
                                <tr style="background-color: rgb(244,252,255);">
                                    <td>PaliGemma-3B</td>
                                    <td>✓</td>
                                    <td>2.9B</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>68.1</td>
                                    <td>614</td>
                                    <td>1686.1</td>
                                    <td>55.2</td>
                                    <td>68.3</td>
                                    <td>34.9</td>
                                    <td>71.0 / 63.6</td>
                                    <td>29.6</td>
                                    <td>33.1</td>
                                    <td>69.6</td>
                                    <td>32.2</td>
                                    <td>28.7</td>
                                </tr>
                                <tr style="background-color: rgb(244,252,255);">
                                    <td>MiniCPM-V</td>
                                    <td>✓</td>
                                    <td>3.4B</td>
                                    <td>38.2</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>60.6</td>
                                    <td>366</td>
                                    <td>1650.2</td>
                                    <td>51.2</td>
                                    <td>56.3</td>
                                    <td>38.3</td>
                                    <td>64.1 / 62.6</td>
                                    <td>41.4</td>
                                    <td>31.1</td>
                                    <td>65.6</td>
                                    <td>36.2</td>
                                    <td>28.9</td>
                                </tr>
                                    <tr style="background-color: rgba(244,252,255, 1);">
                                    <td>MiniCPM-V-2</td>
                                    <td>✓</td>
                                    <td>3.4B</td>
                                    <td>71.9</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>74.1</td>
                                    <td>605</td>
                                    <td>1808.6</td>
                                    <td>55.8</td>
                                    <td>62.9</td>
                                    <td>38.2</td>
                                    <td>69.1 / 66.5</td>
                                    <td>45.3</td>
                                    <td>41.0</td>
                                    <td>67.1</td>
                                    <td>36.1</td>
                                    <td>38.7</td>
                                </tr>

</tr>
                                    <tr style="background-color: rgba(244,252,255, 1);">
                                    <td>Phi-3-vision-128k-instruct</td>
                                    <td>✓</td>
                                    <td>4.2B</td>
                                    <td>-</td>
                                    <td>81.4</td>
                                    <td>-</td>
                                    <td>70.9</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>76.7</td>
                                    <td>40.4</td>
                                    <td>- / -</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>44.5</td>
                                </tr>

                                <tr style="background-color: rgb(255,248,227);">
                                    <td>Mini−InternVL-2B-1.5</td>
                                    <td>✓</td>
                                    <td>2.2B</td>
                                    <td>85.0</td>
                                    <td>74.8</td>
                                    <td>55.4</td>
                                    <td>70.5</td>
                                    <td>654</td>
                                    <td>1901.5</td>
                                    <td>57.9</td>
                                    <td>69.8</td>
                                    <td>34.6</td>
                                    <td>70.9 / 66.2</td>
                                    <td>63.5</td>
                                    <td>39.3</td>
                                    <td>69.8</td>
                                    <td>37.5</td>
                                    <td>41.1</td>
                                </tr>
                                <tr style="background-color: rgb(255,255,255);">
                                    <td colspan="2"><b>Percent of InternVL-1.5</b></td>
                                    <td><b>8.6%</b></td>
                                    <td><b>93.5%</b></td>
                                    <td><b>89.3%</b></td>
                                    <td><b>76.4%</b></td>
                                    <td><b>87.5%</b></td>
                                    <td><b>90.3%</b></td>
                                    <td><b>86.9%</b></td>
                                    <td><b>87.7%</b></td>
                                    <td><b>86.5%</b></td>
                                    <td><b>76.5%</b></td>
                                    <td><b>83.5%</b></td>
                                    <td><b>91.0%</b></td>
                                    <td><b>62.6%</b></td>
                                    <td><b>91.8%</b></td>
                                    <td><b>76.1%</b></td>
                                    <td><b>76.8%</b></td>
                                </tr>


                                <tr style="background-color: rgb(255,248,227);">
                                    <td>Mini−InternVL-4B-1.5</td>
                                    <td>✓</td>
                                    <td>4.2B</td>
                                    <td>87.7</td>
                                    <td>81.0</td>
                                    <td>64.6</td>
                                    <td>72.5</td>
                                    <td>638</td>
                                    <td>2053.6</td>
                                    <td>60.1</td>
                                    <td>76.9</td>
                                    <td>43.3</td>
                                    <td>76.2 / 70.3</td>
                                    <td>58.8</td>
                                    <td>46.7</td>
                                    <td>72.5</td>
                                    <td>42.8</td>
                                    <td>53.7</td>
                                </tr>
                                <tr style="background-color: rgb(255,255,255);">
                                    <td colspan="2"><b>Percent of InternVL-1.5</b></td>
                                    <td><b>16.5%</b></td>
                                    <td><b>96.5%</b></td>
                                    <td><b>96.7%</b></td>
                                    <td><b>89.1%</b></td>
                                    <td><b>90.0%</b></td>
                                    <td><b>88.1%</b></td>
                                    <td><b>93.9%</b></td>
                                    <td><b>91.1%</b></td>
                                    <td><b>95.3%</b></td>
                                    <td><b>95.8%</b></td>
                                    <td><b>89.2%</b></td>
                                    <td><b>84.2%</b></td>
                                    <td><b>74.4%</b></td>
                                    <td><b>95.4%</b></td>
                                    <td><b>86.8%</b></td>
                                    <td><b>100.0%</b></td>
                                </tr>
                            </tbody>
                        </table>
                        <style>
                                .performance table {
                                    font-size: 16px; /* 设置整个表格的字体大小 */
                                }
                                .performance th {
                                    font-size: 14px; /* 设置表头的字体大小 */
                                    padding: 6px!important;  /* 较小的内边距值 */
                                }
                                .performance td {
                                    font-size: 14px; /* 设置表格单元格的字体大小 */
                                    padding: 6px!important;;  /* 较小的内边距值 */
                                }
                            </style>

                        <h3 id="model-card">Model Card</h3>

                        <table  style="text-align: center;">
                          <tr><th colspan="2">Name</th><th>Mini-InternVL-Chat-2B-V1-5</th><th>Mini-InternVL-Chat-4B-V1-5</th></tr>
                          <tr><th rowspan="4">Model Size</th><td>Total</td><td><b>2.21B</b></td><td><b>4.15B</b></td></tr>
                          <tr><td>ViT</td><td>304.01M</td><td>304.01M</td></tr>
                          <tr><td>MLP</td><td>12.60M</td><td>22.03M</td></tr>
                          <tr><td>LLM</td><td>1.89B</td><td>3.82B</td></tr>
                          <tr><th colspan="2">Resolution</th><td colspan="2" >dynamic resolution, max to 12 tiles of 448 × 448 in training, max to 40 tiles in testing (4K resolution).</td></tr>
                            <tr><th rowspan="2">Stage-1</th><th>Training Data</th><td colspan="2"  style="text-align: left;">The pre-training dataset utilized in our InternVL 1.5 encompasses a diverse range of publicly accessible sources. These datasets span multiple tasks, including captioning, which predominantly uses datasets such as Laion-EN, Laion-ZH, COYO, and GRIT, constituting 53.9% of the total data. Detection and grounding tasks utilize datasets like Objects365, GRIT, and All-Seeing, making up 5.2%. For OCR tasks, we utilized large-scale datasets such as Wukong-OCR, LaionCOCO-OCR, and Common Crawl PDFs, which constitute 32.0% of our data. These datasets were constructed using PaddleOCR to perform OCR on Chinese images from Wukong and on English images from LaionCOCO. Smaller OCR datasets include MMC-Inst, LSVT, ST-VQA, RCTW-17, ArT, and others, accounting for 8.9% of the data, which focus on more specific or constrained OCR challenges.</td></tr>
                          <tr><th>Trainable Module</th><td colspan="1">ViT + MLP</td><td colspan="1">MLP</td></tr>
                          <tr><th rowspan="2">Stage-2</th><th>Training Data</th><td colspan="2">5M high-quality bilingual data. Please see the technical report of InternVL 1.5 for more details.</td></tr>
                          <tr><th>Trainable Module</th><td colspan="2">ViT + MLP + LLM</td></tr>
                        </table>

                        <p>The hyperparameters used for pre-training and fine-tuning are listed in the following table.</p>

                        <table>
                            <thead>
                                <tr>
                                    <th>Model Size</th>
                                    <th>Stage</th>
                                    <th>Trainable Module</th>
                                    <th>#Samples</th>
                                    <th>Batch Size</th>
                                    <th>Learning rate</th>
                                    <th>Epoch</th>
                                    <th>Max length</th>
                                    <th>Weight decay</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td rowspan="2">2B</td>
                                    <td>Pre-train</td>
                                    <td>ViT + MLP</td>
                                    <td>~200M</td>
                                    <td>2048</td>
                                    <td>2e-5</td>
                                    <td>1</td>
                                    <td>4096</td>
                                    <td>0.01</td>
                                </tr>
                                <tr>
                                    <td>Fine-tune</td>
                                    <td>ViT + MLP + LLM (full model)</td>
                                    <td>~5M</td>
                                    <td>1024</td>
                                    <td>4e-5</td>
                                    <td>1</td>
                                    <td>8192</td>
                                    <td>0.01</td>
                                </tr>
                            <tr>
                                    <td rowspan="2">4B</td>
                                    <td>Pre-train</td>
                                    <td>MLP</td>
                                    <td>~24M</td>
                                    <td>2048</td>
                                    <td>2e-4</td>
                                    <td>1</td>
                                    <td>4096</td>
                                    <td>0.05</td>
                                </tr>
                                <tr>
                                    <td>Fine-tune</td>
                                    <td>ViT + MLP + LLM (full model)</td>
                                    <td>~5M</td>
                                    <td>1024</td>
                                    <td>4e-5</td>
                                    <td>1</td>
                                    <td>8192</td>
                                    <td>0.05</td>
                                </tr>
                            </tbody>
                        </table>

                    </div>
                    <h2 class="title">Citation</h2>
<pre><code>
  @article{chen2023internvl,
      title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks},
      author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and Li, Bin and Luo, Ping and Lu, Tong and Qiao, Yu and Dai, Jifeng},
      journal={arXiv preprint arXiv:2312.14238},
      year={2023}
  }
  @article{chen2024far,
    title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},
    author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
    journal={arXiv preprint arXiv:2404.16821},
    year={2024}
  }
  </code></pre>
                </article>
            </div>
        </main>
        <footer class="site-footer h-card">
            <data class="u-url" href="/blog/"></data>
            <div class="wrapper">
                <p class="footer-colophon">
                </p>
            </div>
        </footer>
    </body>
</html>
