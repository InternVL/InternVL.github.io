<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <!-- Begin Jekyll SEO tag v2.8.0 -->
        <title>InternOmni:Extending InternVL with Audio Modality</title>
        <meta name="generator" content="Jekyll v3.9.4"/>
        <meta property="og:title" content="InternOmni"/>
        <meta name="author" content="cuierfei"/>
        <meta property="og:locale" content="en_US"/>
        <meta property="og:site_name" content="InternOmni"/>
        <meta property="og:type" content="article"/>
        <meta property="article:published_time" content="2024-01-30T12:33:38-06:00"/>
        <meta name="twitter:card" content="summary"/>
        <meta property="twitter:title" content="InternOmni"/>
        <!-- End Jekyll SEO tag -->
        <link rel="icon" href="https://github-production-user-asset-6210df.s3.amazonaws.com/47669167/330728723-7037290e-f474-4d11-b90f-1d8316087bf8.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240529%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240529T072300Z&X-Amz-Expires=300&X-Amz-Signature=d12b9e5c3c49a082747f5da55529a4f1247cd17b4329fafc1cb6d1c0678efa77&X-Amz-SignedHeaders=host&actor_id=23737120&key_id=0&repo_id=721995615">
        <link rel="stylesheet" href="/blog/assets/main.css">
        <link type="application/atom+xml" rel="alternate" href="https://llava-vl.github.io/blog/feed.xml" title="LLaVA"/>
    </head>
    <body>
        <header class="site-header" role="banner">
            <div class="wrapper">
                <a class="site-title" rel="author" href="/blog/">InternVL</a>
                <nav class="site-nav">
                    <input type="checkbox" id="nav-trigger" class="nav-trigger"/>
                    <label for="nav-trigger">
                        <span class="menu-icon">
                            <svg viewBox="0 0 18 15" width="18px" height="15px">
                                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
                            </svg>
                        </span>
                    </label>
                    <div class="trigger"></div>
                </nav>
            </div>
        </header>
        <main class="page-content" aria-label="Content">
            <style>
                @media (max-width: 768px) {
                        img.responsive {
                        width: 100% !important;
                    }
                }
            </style>
            <div class="wrapper">
                <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
                    <header class="post-header">
                        <h1 class="post-title p-name" itemprop="name headline">InternOmniÔºö Extending InternVL with Audio Modality</h1>
                        <p class="post-meta">
                            <time class="dt-published" datetime="2024-01-30T12:33:38-06:00" itemprop="datePublished">2024/07/31
      </time>
                            ‚Ä¢
                            <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                                <span class="p-author h-card" itemprop="name">OpenGVLab
                                </span>
                            </span>
                        </p>
                    </header>

                    <p><a rel="nofollow" href="../">[üÜï Go Back]</a>  <a rel="nofollow" href="https://arxiv.org/abs/2312.14238">[üìú InternVL 1.0 Paper]</a>  <a rel="nofollow" href="https://arxiv.org/abs/2404.16821">[üìú InternVL 1.5 Report]</a>  <a rel="nofollow" href="https://internvl.opengvlab.com/">[üó®Ô∏è Chat Demo]</a>  <a rel="nofollow" href="https://huggingface.co/spaces/OpenGVLab/InternVL">[ü§ó HF Demo]</a>  <a rel="nofollow" href="https://modelscope.cn/models/OpenGVLab/Mini-InternVL-Chat-4B-V1-5/summary">[<img src="images/modelscope_logo.png" width="20px" style="max-width: 100%;"> ModelScope]</a>  <a rel="nofollow" href="https://github.com/OpenGVLab/InternVL?tab=readme-ov-file#quick-start-with-huggingface">[üöÄ Quick Start]</a> <a rel="nofollow" href="https://zhuanlan.zhihu.com/p/675877376">[üìñ ‰∏≠ÊñáËß£ËØª]</a></p>


                    <div class="post-content e-content" itemprop="articleBody">
                        <!-- ÊèíÂÖ•MarkdownËΩ¨Êç¢ÂêéÁöÑHTMLÂÜÖÂÆπ -->

                        <table>
                            <thead>
                                <tr>
                                    <th>Type</th>
                                    <th>Model</th>
                                    <th>Date</th>
                                    <th>Download</th>
                                    <th>Note</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Multimodal Large Language Models</td>
                                    <td>InternOmni</td>
                                    <td>2024.07.25</td>
                                    <td>Todo</td>
                                    <td>Extending InternVL's Modalities to Audio with Good Performance</td>
                                </tr>
                                <tr>
                                    <td>Vision Foundation Model</td>
                                    <td>InternViT-300M-448px</td>
                                    <td>2024.05.25</td>
                                    <td>ü§ó <a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px" rel="nofollow">HF link</a></td>
                                    <td>Distilled small vision foundation model with 300M parameters.</td>
                                </tr>
                                <tr>
                                    <td>Audio Foundation Model</td>
                                    <td>Whisper-large-v3</td>
                                    <td>2024.07.25</td>
                                    <td>ü§ó <a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px" rel="nofollow">HF link</a></td>
                                    <td>Pre-trained model for ASR and speech translation</td>
                                </tr>
                            </tbody>
                        </table>

                        <h2>InternOmni</h2>

                        <h3>Method</h3>

                        <p>We introduce InternOmni, an upcoming open-source multimodal large language model that adds audio input to the existing InternVL series. The goal is to enhance the modality of the InternVL series models and move further toward general artificial intelligence while providing users with a better experience. We employ the following designs:</p>
                        <ol>
                            <li><strong>Strong Vision Encoder:</strong> Building on the previously applied vision model InternViT-6B, we utilized distillation to create a lightweight vision foundation model, InternViT-300M. This enhances visual understanding while reducing the model size.</li>
                            <li><strong>Efficient Audio Encoder:</strong> We adopted OpenAI's open-source Whisper-large-v3 model, which has been trained on a large amount of audio data and has strong capabilities in speech recognition and translation.</li>
                            <li><strong>High-Quality Audio-Image Dataset:</strong> We carefully collected a high-quality audio-image dataset that covers common scenes and document images. This dataset is used for audio question-answering training on images, improving the model's performance in audio question-answering tasks.</li>
                        </ol>
                        <div style="text-align: center;">
                            <img src="./images/structure.png" alt="Model structure" style="width:100%; max-width:800px;">
                        </div>

                        <h3 id="model-card">Model Card</h3>

                        <table style="text-align: center;">
                          <tr><th colspan="2">Name</th><th>InternOmni</th></tr>
                          <tr><th rowspan="2">Model Size</th><td>Total</td><td><b>8.73B</b></td></tr>
                          <tr><td>LLM</td><td>7.74B</td></tr>
                          <tr><th colspan="2">Resolution</th><td>448 √ó 448</td></tr>
                            <tr><th rowspan="3"><nobr>Stage-1</nobr></th><th>Training Data</th><td>To ensure the proper alignment of audio data, we train on approximately 26 million data points, including datasets like GigaSpeech, CommonVoice, Libriheavy, and WENETSPEECH. The format used is: audio+text => text. At this stage, we freeze the ViT and its MLP, only keeping the audio-related components active.</td></tr>
                          <tr><th>Trainable Module</th><td>MLP_audio</td></tr>
                          <tr><th>Trainable Cost</th><td>64 GPUs, 4k steps, approximately 30 hours.</td></tr>
                            <tr><th rowspan="3">Stage-2</th><th>Training Data</th><td>We train on approximately 1.9 million open-source image-text instruction datasets, replacing the original text with audio. These datasets include TextVQA, GQA, OKVQA, ALLAVA, and others. The format used is audio+image => text. At this stage, we freeze both the ViT and Whisper encoders, only keeping the MLP layers used for alignment activ </td></tr>
                          <tr><th>Trainable Module</th><td>MLP_audio</td></tr>
                          <tr><th>Trainable Cost</th><td>32 GPUs, 3k steps, approximately 15 hours.</td></tr>
                        </table>

                        <h3>Performance</h3>

                        <p>InternVL Omni did not use VL data for training in both the alignment and SFT stages; instead, it was trained entirely with audio data. However, it retained InternVL's powerful capabilities in handling complex image-text data, excelling in tasks such as scientific charts, general charts, documents, infographics, and OCR.</p>


                        <table>
                            <thead>
                              <tr>
                                <th>Model</th>
                                <th>Model size</th>
                                <th>MMMU (val)</th>
                                <th>MMB-en (test)</th>
                                <th>MMB-CN (test)</th>
                                <th>TextVQA (val)</th>
                                <th>SEED (image)</th>
                                <th>DocVQA (val)</th>
                                <th>AI2D (test)</th>
                                <th>ChartVQA (test)</th>
                                <th>Infovqa (val)</th>
                                <th>MMVet</th>
                              </tr>
                            </thead>
                            <tbody>
                              <tr>
                                <td>Qwen-VL-Chat</td>
                                <td>9.6B</td>
                                <td>37</td>
                                <td>-</td>
                                <td>-</td>
                                <td>60.7</td>
                                <td>64.8</td>
                                <td>-</td>
                                <td>63</td>
                                <td>-</td>
                                <td>-</td>
                                <td>47.3</td>
                              </tr>
                              <tr>
                                <td>LLaVA-1.5-7B</td>
                                <td>7.2B</td>
                                <td>33.7</td>
                                <td>62.3</td>
                                <td>56</td>
                                <td>45.5</td>
                                <td>66.4</td>
                                <td>-</td>
                                <td>55.9</td>
                                <td>-</td>
                                <td>-</td>
                                <td>32.2</td>
                              </tr>
                              <tr>
                                <td>InternLM-XComposer</td>
                                <td>8B</td>
                                <td>35.6</td>
                                <td>6.4</td>
                                <td>45.5</td>
                                <td>38.5</td>
                                <td>66.1</td>
                                <td>-</td>
                                <td>56.9</td>
                                <td>-</td>
                                <td>-</td>
                                <td>35.2</td>
                              </tr>
                              <tr>
                                <td>LLaVA-1.5-13B</td>
                                <td>13.4B</td>
                                <td>37</td>
                                <td>65.8</td>
                                <td>62.1</td>
                                <td>48.9</td>
                                <td>68.2</td>
                                <td>-</td>
                                <td>61.1</td>
                                <td>-</td>
                                <td>-</td>
                                <td>35.6</td>
                              </tr>
                              <tr>
                                <td>Cambrian-8B</td>
                                <td>8B</td>
                                <td>41.8</td>
                                <td>71.6</td>
                                <td>64.9</td>
                                <td>72.6</td>
                                <td>73.3</td>
                                <td>-</td>
                                <td>74.6</td>
                                <td>-</td>
                                <td>-</td>
                                <td>48</td>
                              </tr>
                              <tr>
                                <td>InternVL2-8B</td>
                                <td>8B</td>
                                <td>51.2</td>
                                <td>81.7</td>
                                <td>81.2</td>
                                <td>77.4</td>
                                <td>76.2</td>
                                <td>90.8</td>
                                <td>83.6</td>
                                <td>83.8</td>
                                <td>72.6</td>
                                <td>60</td>
                              </tr>
                              <tr style="background-color: rgb(255,248,227);">
                                <td><nobr>InternOmni</nobr></td>
                                <td>8.73B</td>
                                <td>48.4</td>
                                <td>76.6</td>
                                <td>76.9</td>
                                <td>69.1</td>
                                <td>71.1</td>
                                <td>85.3</td>
                                <td>83.3</td>
                                <td>74.4</td>
                                <td>66.4</td>
                                <td>62.3</td>
                              </tr>
                            </tbody>
                          </table>

                        <h3>Benchmark</h3>

                        <p>Existing audio benchmarks mainly focus on the audio itself, with relatively simple questions and images. To better evaluate the model's ability to handle complex audio-image pair problems, especially those involving tables and mathematical knowledge, I transcribed the text parts of some existing complex image-text VQA datasets and converted them into audio files. From these, I selected the more challenging data from the original image-text questions, creating a 15k audio-image question-answer dataset. The benchmark will be open-sourced soon.The format of the benchmark is as follows:</p>

                        <pre><code>
                            {"image": "data/images/fa9ffd5aca1e4e51.jpg", "question": "audio_1.wav", "question_id": 1, "category": "ChartVQA"}
                            {"image": "data/images/shsiishslla54ssc.jpg", "question": "audio_2.wav", "question_id": 2, "category": "TextVQA"}
                            {"image": "data/images/shiqsiqhisnnkscns.jpg", "question": "audio_3.wav", "question_id": 3, "category": "OCRVQA"}
                            {"image": "data/images/fjqsxdjjodowdddwd.jpg", "question": "audio_4.wav", "question_id": 4, "category": "MATHVQA"}
                            {"image": "data/images/cncnoqdhgbooqjsjj.jpg", "question": "audio_5.wav", "question_id": 5, "category": "ChartVQA"}
                            {"image": "data/images/ssq4fyj66jj5hhg5g.jpg", "question": "audio_6.wav", "question_id": 6, "category": "OCRVQA"}
                            {"image": "data/images/ffjsdoggh5hghf6hf.jpg", "question": "audio_7.wav", "question_id": 7, "category": "MATHVQA"}
                            {"image": "data/images/fjdogjbnddggwofhf.jpg", "question": "audio_8.wav", "question_id": 8, "category": "TextVQA"}
                        </code></pre>

                        <h3>Examples</h3>
                        <div class="slider-container">
                            <div class="slider">
                                <div class="slider-inner">
                                    <div class="slide">
                                        <div class="slide-content">
                                            <h3>Input:</h3>
                                            <div class="input-box">
                                                <p>[Èü≥È¢ëÊñáÊú¨] Is it possible to make a left turn from this lane?</p>
                                                <audio controls>
                                                    <source src="./audios/00045.wav" type="audio/wav">
                                                    ÊÇ®ÁöÑÊµèËßàÂô®‰∏çÊîØÊåÅÈü≥È¢ëÂÖÉÁ¥†„ÄÇ
                                                </audio>
                                                <img src="./images/00045.jpg" alt="Input Image" style="max-width:100%; height:auto; margin-top:10px;">
                                            </div>
                                            <h3>InternOmni:</h3>
                                            <div class="output-box">
                                                <p>No, the lane you're currently in seems to be a straight-only lane. However, there is a left turn signal at the next intersection, indicating that you can make a left turn from your current position.</p>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="slide">
                                        <div class="slide-content">
                                            <h3>Input:</h3>
                                            <div class="input-box">
                                                <p>[Èü≥È¢ëÊñáÊú¨] ÂÜô‰∏Ä‰∏™ÂØìË®ÄÊïÖ‰∫ã</p>
                                                <audio controls>
                                                    <source src="./audios/00082.mp3" type="audio/mp3">
                                                    ÊÇ®ÁöÑÊµèËßàÂô®‰∏çÊîØÊåÅÈü≥È¢ëÂÖÉÁ¥†„ÄÇ
                                                </audio>
                                                <img src="./images/00082.png" alt="Input Image" style="max-width:100%; height:auto; margin-top:10px;">
                                            </div>
                                            <h3>InternOmni:</h3>
                                            <div class="output-box">
                                                <p>‰ªéÂâçÔºåÊúâ‰∏Ä‰∏™Âπ¥ËΩª‰∫∫Âè´Êùé‰∫ë„ÄÇ‰ªñÈùûÂ∏∏Âã§Â•ãÂ•ΩÂ≠¶Ôºå‰ΩÜÊòØÊÄªÊòØÊ≤°ÊúâÊú∫‰ºöÂ±ïÁ§∫Ëá™Â∑±ÁöÑÊâçÂçéÂíåËÉΩÂäõ„ÄÇÊúâ‰∏ÄÂ§©Ôºå‰ªñÂê¨ËØ¥ÈôÑËøëÁöÑÂ±±‰∏äÊúâ‰∏ÄÂè™Á•ûÂ•áÁöÑÈ∏°ÔºåÂè™Ë¶ÅËÉΩÂ§üÂæóÂà∞ÂÆÉÁöÑÁæΩÊØõÔºåÂ∞±ËÉΩËé∑ÂæóÊàêÂäüÂíåË¥¢ÂØå„ÄÇ‰∫éÊòØÔºå‰ªñÂÜ≥ÂÆöÂéªÂØªÊâæËøôÂè™Á•ûÂ•áÁöÑÈ∏°„ÄÇ...</p>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="nav-buttons">
                                    <button class="nav-button" onclick="slideLeft()">&#10094;</button>
                                    <button class="nav-button" onclick="slideRight()">&#10095;</button>
                                </div>
                            </div>
                        </div>
                        
                        <script>
                            let currentIndex = 0;
                        
                            function slideLeft() {
                                if (currentIndex > 0) {
                                    currentIndex--;
                                    document.querySelector('.slider-inner').style.transform = `translateX(-${currentIndex * 100}%)`;
                                }
                            }
                        
                            function slideRight() {
                                if (currentIndex < document.querySelectorAll('.slide').length - 1) {
                                    currentIndex++;
                                    document.querySelector('.slider-inner').style.transform = `translateX(-${currentIndex * 100}%)`;
                                }
                            }
                        </script>
                        
                        <style>
                            .slider-container {
                                display: flex;
                                justify-content: center;
                                align-items: center;
                                margin-top: 20px;
                            }
                        
                            .slider {
                                width: 100%;
                                max-width: 800px;
                                overflow: hidden;
                                position: relative;
                                border: 1px solid #ddd;
                                border-radius: 8px;
                                box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.1);
                                background-color: #f9f9f9;
                            }
                        
                            .slider-inner {
                                display: flex;
                                transition: transform 0.5s ease;
                            }
                        
                            .slide {
                                min-width: 100%;
                                box-sizing: border-box;
                                padding: 20px;
                            }
                        
                            .slide-content {
                                max-width: 800px;
                                margin: 0 auto;
                            }
                        
                            .input-box, .output-box {
                                background-color: #ffffff;
                                border-radius: 8px;
                                padding: 15px;
                                box-shadow: 0px 2px 5px rgba(0, 0, 0, 0.1);
                                margin-bottom: 15px;
                            }
                        
                            .input-box p, .output-box p {
                                margin: 0;
                            }
                        
                            .nav-buttons {
                                position: absolute;
                                top: 50%;
                                transform: translateY(-50%);
                                width: 100%;
                                display: flex;
                                justify-content: space-between;
                            }
                        
                            .nav-button {
                                background: rgba(0, 0, 0, 0.5);
                                color: white;
                                border: none;
                                padding: 10px;
                                cursor: pointer;
                                border-radius: 50%;
                                outline: none;
                            }
                        
                            .nav-button:focus {
                                outline: none;
                            }
                        </style>
                        



                        <h3 class="title">Citation</h3>
<pre><code>
  @article{chen2023internvl,
      title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks},
      author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and Li, Bin and Luo, Ping and Lu, Tong and Qiao, Yu and Dai, Jifeng},
      journal={arXiv preprint arXiv:2312.14238},
      year={2023}
  }
  @article{chen2024far,
    title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},
    author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
    journal={arXiv preprint arXiv:2404.16821},
    year={2024}
  }
  </code></pre>
                        <br><h4 class="title"><a href="../">üîô Go Back</a></h4>
</div>


                </article>
            </div>
        </main>
        <footer class="site-footer h-card">
        <!--   <data class="u-url" href="https://internvl.github.io/blog/">InternVL</data>--> 
            <p class="footer-colophon">
            </p>
        </footer>
    </body>
</html>
