<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <!-- Begin Jekyll SEO tag v2.8.0 -->
        <title>Mini-InternVL 2.0: A Flexible-Transfer Pocket Multimodal Model with 5% Parameters and 90% Performance</title>
        <meta name="generator" content="Jekyll v3.9.4"/>
        <meta name="author" content="Zhangwei"/>
        <meta property="og:locale" content="en_US"/>
        <meta property="og:site_name" content="InternVL"/>
        <meta property="og:type" content="article"/>
        <meta property="article:published_time" content="2024-01-30T12:33:38-06:00"/>
        <meta name="twitter:card" content="summary"/>
        <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.35.2/gradio.js"></script>
        <!-- End Jekyll SEO tag -->
        <link rel="icon" href="https://github-production-user-asset-6210df.s3.amazonaws.com/47669167/330728723-7037290e-f474-4d11-b90f-1d8316087bf8.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240529%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240529T072300Z&X-Amz-Expires=300&X-Amz-Signature=d12b9e5c3c49a082747f5da55529a4f1247cd17b4329fafc1cb6d1c0678efa77&X-Amz-SignedHeaders=host&actor_id=23737120&key_id=0&repo_id=721995615">
        <link rel="stylesheet" href="/blog/assets/main.css">
        <link type="application/atom+xml" rel="alternate" href="https://llava-vl.github.io/blog/feed.xml" title="LLaVA"/>
    </head>
    <body>
        <header class="site-header" role="banner">
            <div class="wrapper">
                <a class="site-title" rel="author" href="/blog/">InternVL</a>
                <nav class="site-nav">
                    <input type="checkbox" id="nav-trigger" class="nav-trigger"/>
                    <label for="nav-trigger">
                        <span class="menu-icon">
                            <svg viewBox="0 0 18 15" width="18px" height="15px">
                                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
                            </svg>
                        </span>
                    </label>
                    <div class="trigger"></div>
                </nav>
            </div>
        </header>
        <main class="page-content" aria-label="Content">
            <div class="wrapper">
                <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
                    <header class="post-header">
                        <h1 class="post-title p-name" itemprop="name headline">Mini-InternVL 2.0: A Flexible-Transfer Pocket Multimodal Model with 5% Parameters and 90% Performance</h1>
                        <p class="post-meta">
                            <time class="dt-published" datetime="2024-05-20" itemprop="datePublished">2024/10/21</time>
                            • 
                            <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                                <span class="p-author h-card" itemprop="name">Zhangwei Gao*, Zhe Chen*, Erfei Cui*, Yiming Ren*, Weiyun Wang*, Jinguo Zhu, Hao Tian, Shenglong Ye, Junjun He, Xizhou Zhu, Lewei Lu, Tong Lu, Yu Qiao, Jifeng Dai, Wenhai Wang (* Equal Contribution)</span>
                            </span>
                        </p>
                    </header>

                    <p><a rel="nofollow" href="../">[🆕 Go Back]</a>  <a rel="nofollow" href="https://arxiv.org/abs/2410.16261">[📜 Mini-InternVL Report]</a>  <a rel="nofollow" href="https://arxiv.org/abs/2404.16821">[📜 InternVL 1.5 Report]</a>  <a rel="nofollow" href="https://internvl.readthedocs.io/en/latest/internvl2.0/domain_adaptation.html">[📖 Document]</a>  <a rel="nofollow" href="https://modelscope.cn/models/OpenGVLab/InternVL2-4B">[<img src="images/modelscope_logo.png" width="20px" style="max-width: 100%;"> ModelScope]</a>  <a rel="nofollow" href="https://github.com/OpenGVLab/InternVL?tab=readme-ov-file#quick-start-with-huggingface">[🚀 Quick Start]</a> <a rel="nofollow" href="https://zhuanlan.zhihu.com/p/700942703">[📖 中文解读]</a></p>

                    <p><strong><span style="color: red;">5% of the model size, 90% of the performance;</span></strong></p>

                    <table>
                        <thead>
                        <tr>
                        <th>Type</th>
                        <th>Model</th>
                        <th>Download</th>
                        <th>Note</th>
                        </tr>
                        </thead>
                        <tbody>

                        <tr>
                        <td rowspan="3">Vision Large Language Model</td>
                        <td>Mini-InternVL2-1B</td>
                        <td>🤗 <a href="https://huggingface.co/OpenGVLab/InternVL2-1B" rel="nofollow">HF link</a></td>
                        <td>🚀 Only 1B parameters, anyone can deploy it locally.</td>
                        </tr>
                        <tr>
                        <td>Mini-InternVL2-2B</td>
                        <td>🤗 <a href="https://huggingface.co/OpenGVLab/InternVL2-2B" rel="nofollow">HF link</a></td>
                        <td>🚀 Only 2B parameters, anyone can deploy it locally.</td>
                        </tr>
                        <tr>
                            <td>Mini-InternVL2-4B</td>

                            <td>🤗 <a href="https://huggingface.co/OpenGVLab/InternVL2-4B" rel="nofollow">HF link</a></td>
                            <td>🚀 5% of the model size, 90% of the performance.</td>
                            </tr>
                        <tr>
                         <td>Vision Foundation Model</td> 
                            <td>InternViT-300M-448px</td>
                        <td>🤗 <a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px" rel="nofollow">HF link</a></td>
                        <td>Distilled small vision foundation model with 300M parameters.</td>
                        </tr>

                        </tbody>
                    </table>
                    <p>We introduce <strong>Mini-InternVL 2.0</strong>, a series of MLLMs with parameters ranging from 1B to 4B, which achieves 90% of the performance with only 5% of the parameters. This significant improvement in efficiency and effectiveness makes our models more accessible and applicable in various real-world scenarios.
                    </p>
                    <p>We employ  <strong>InternViT-300M</strong> as our visual encoder, a lightweight vision model that inherits the capabilities of a powerful vision encoder. We directly leverage <a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5">InternViT-6B</a> that has undergone generative training on diverse datasets to transfer knowledge to a lightweight vision model, <a href="https://huggingface.co/openai/clip-vit-large-patch14-336"> CLIP-ViT-L-336px</a>. Our <a  href="https://arxiv.org/abs/2410.16261">Report</a> contains a more detailed introduction.. 

                    </p>
                    
                    
                    <div class="post-content e-content" itemprop="articleBody">
                        <style>
                            @media (max-width: 768px) {
                                    img.responsive {
                                    width: 100% !important;
                                }
                            }
                        </style>
                        <center>
                            <p><img class="responsive" style="width: 80%;" alt="image" src="images/mini-internvl2.png"><br></p>
                        </center>
                

                        <style>
                            .performance thead th
                        {
                            position: sticky;
                            top: 0px;
                        }
                        </style>

                    <h3 id="performance">Performance</h3>
                    <p>Mini-InternVL 2.0 demonstrates strong performance across the majority of benchmarks.
                        Our smallest model contains only 1 billion parameters, yet it demonstrates performance comparable to 2 billion parameter models, such as DeepSeek-VL-1.3B and MiniCPM-V 2.0. Compared to other lightweight models, our Mini-InternVL2-4B excels across most benchmarks, particularly in MMbench, ChartQA, DocVQA, and MathVista, where its performance is on par with commercial models like Gemini-Pro-1.5.
                        Notably, compared to InternVL2-Llama3-76B, which utilizes the larger InternViT-6B, Mini-InternVL 2.0 achieves approximately 90% of its performance while using 5% parameters. This highlights the effectiveness of our knowledge distillation strategy.</p>
                       <table>
                        <thead>
                            <tr>
                                <th>name</th>
                                <th>MMMU<br>(val)</br></th>
                                <th>MathVista<br>(testmini)</br></th>
                                <th>AI2D<br>(test)</br></th>
                                <th>ChartQA<br>(test)</br></th>
                                <th>DocVQA<br>(test)</br></th>
                                <th>InfoVQA<br>(test)</br></th>
                                <th>OCRBench</th>
                                <th>MMB-EN<br>(test)</br></th>
                                <th>MMB-CN<br>(test)</br></th>
                                <th>OpenCompass<br>(avg score)</br></th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>GPT-4V*<br>(20240409)</td>
                                <td>61.7</td>
                                <td>58.1</td>
                                <td>89.4</td>
                                <td>78.1</td>
                                <td>87.2</td>
                                <td>-</td>
                                <td>678</td>
                                <td>81.0</td>
                                <td>80.2</td>
                                <td>63.5</td>
                            </tr>
                            <tr>
                                <td>Gemini Pro 1.5*</td>
                                <td>60.6</td>
                                <td>57.7</td>
                                <td>80.3</td>
                                <td>81.3</td>
                                <td>86.5</td>
                                <td>72.7</td>
                                <td>754</td>
                                <td>73.9</td>
                                <td>73.8</td>
                                <td>64.4</td>
                            </tr>
                            <tr>
                                <td>Claude3.5-Sonnet*</td>
                                <td>65.9</td>
                                <td>67.7</td>
                                <td>94.7</td>
                                <td>90.8</td>
                                <td>95.2</td>
                                <td>-</td>
                                <td>788</td>
                                <td>79.7</td>
                                <td>80.7</td>
                                <td>67.9</td>
                            </tr>
                            <tr>
                                <td>GPT-4o*<br>(20240513)</td>
                                <td>69.2</td>
                                <td>63.8</td>
                                <td>94.2</td>
                                <td>85.7</td>
                                <td>92.8</td>
                                <td>-</td>
                                <td>736</td>
                                <td>83.4</td>
                                <td>82.1</td>
                                <td>69.9</td>
                            </tr>
                            <tr>
                                <td>Cambrian-1</td>
                                <td>50.4</td>
                                <td>53.2</td>
                                <td>79.7</td>
                                <td>75.6</td>
                                <td>75.5</td>
                                <td>-</td>
                                <td>600</td>
                                <td>81.4</td>
                                <td>-</td>
                                <td>58.3</td>
                            </tr>

                            <tr>
                                <td>LLaVA-NeXT Qwen1.5</td>
                                <td>50.1</td>
                                <td>49.0</td>
                                <td>80.4</td>
                                <td>79.7</td>
                                <td>85.7</td>
                                <td>-</td>
                                <td>-</td>
                                <td>80.5</td>
                                <td>-</td>
                                <td>-</td>
                            </tr>
                            <tr>
                                <td>DeepSeek-VL-1.3B</td>
                                <td>33.8</td>
                                <td>29.8</td>
                                <td>51.5</td>
                                <td>-</td>
                                <td>-</td>
                                <td>-</td>
                                <td>413</td>
                                <td>64.6</td>
                                <td>62.9</td>
                                <td>39.6</td>
                            </tr>
                            <tr>
                                <td>MiniCPM-V 2.0</td>
                                <td>38.2</td>
                                <td>38.7</td>
                                <td>62.9</td>
                                <td>-</td>
                                <td>71.9</td>
                                <td>-</td>
                                <td>605</td>
                                <td>69.1</td>
                                <td>66.5</td>
                                <td>47.9</td>
                            </tr>
                            <tr>
                                <td>Qwen2-VL-2B</td>
                                <td>42.2</td>
                                <td>43.0</td>
                                <td>74.7</td>
                                <td>73.5</td>
                                <td>90.1</td>
                                <td>65.5</td>
                                <td>794</td>
                                <td>74.9</td>
                                <td>73.5</td>
                                <td>57.2</td>
                            </tr>
                            <tr>
                                <td>InternVL2-Llama3-76B</td>
                                <td>58.2</td>
                                <td>65.5</td>
                                <td>87.6</td>
                                <td>88.4</td>
                                <td>94.1</td>
                                <td>82.0</td>
                                <td>839</td>
                                <td>86.5</td>
                                <td>86.3</td>
                                <td>71.0</td>
                            </tr>

                            <tr>
                                <td>Mini-InternVL2-1B</td>
                                <td>36.7</td>
                                <td>37.7</td>
                                <td>64.1</td>
                                <td>72.9</td>
                                <td>81.7</td>
                                <td>50.9</td>
                                <td>754</td>
                                <td>65.4</td>
                                <td>60.7</td>
                                <td>48.3</td>
                            </tr>
                            <tr>
                                <td>Mini-InternVL2-2B</td>
                                <td>36.3</td>
                                <td>46.3</td>
                                <td>74.1</td>
                                <td>76.2</td>
                                <td>86.9</td>
                                <td>58.9</td>
                                <td>784</td>
                                <td>73.2</td>
                                <td>70.9</td>
                                <td>54.0</td>
                            </tr>
                            <tr>
                                <td>Mini-InternVL2-4B</td>
                                <td>48.3</td>
                                <td>58.6</td>
                                <td>78.9</td>
                                <td>81.5</td>
                                <td>89.2</td>
                                <td>67.0</td>
                                <td>788</td>
                                <td>78.6</td>
                                <td>73.9</td>
                                <td>60.6</td>
                            </tr>
                        </tbody>
                    </table>
                    <ul>
                        <li>We simultaneously use <a href="https://github.com/OpenGVLab/InternVL/tree/v2.0.0/internvl_chat/eval">InternVL</a> and <a href="https://github.com/open-compass/VLMEvalKit">VLMEvalKit</a> repositories for model evaluation. Specifically, the results reported for AI2D, ChartQA, DocVQA, InfoVQA, MMBench were tested using the InternVL repository. MMMU, MathVista and OCRBench were evaluated using the VLMEvalKit.</li>
                      <li>Please note that evaluating the same model using different testing toolkits like InternVL and VLMEvalKit can result in slight differences, which is normal. Updates to code versions and variations in environment and hardware can also cause minor discrepancies in results.</li>
                    </ul>
                
                        <style>
                                .performance table {
                                    font-size: 16px; /* 设置整个表格的字体大小 */
                                }
                                .performance th {
                                    font-size: 14px; /* 设置表头的字体大小 */
                                    padding: 6px!important;  /* 较小的内边距值 */
                                } 
                                .performance td {
                                    font-size: 14px; /* 设置表格单元格的字体大小 */
                                    padding: 6px!important;;  /* 较小的内边距值 */
                                }
                            </style>

                        <h3 id="model-card">Model Card</h3>

                        <table  style="text-align: center;">
                          <tr><th colspan="2">Name</th><th>Mini-InternVL2-1B</th><th>Mini-InternVL2-2B</th><th>Mini-InternVL2-4B</th></tr>
                          <tr><th rowspan="4">Model Size</th><td>Total</td><td><b>0.94B</b></td><td><b>2.21B</b></td><td><b>4.15B</b></td></tr>
                          <tr><td>ViT</td><td>304.01M</td><td>304.01M</td><td>304.01M</td></tr>
                          <tr><td>MLP</td><td>4.48M</td><td>12.60M</td><td>22.03M</td></tr>
                          <tr><td>LLM</td><td>0.63B</td><td>1.89B</td><td>3.82B</td></tr>
                          <tr><th colspan="2">Resolution</th><td colspan="3" >dynamic resolution, max to 12 tiles of 448 × 448 in training, max to 40 tiles in testing (4K resolution).</td></tr>
                            <tr><th rowspan="2"><nobr>Stage-1</nobr></th><th>Training Data</th><td colspan="3"  style="text-align: left;"> We entend the pre-training dataset used in InternVL 1.5
                                with data collected from diverse sources.
                                These datasets span multiple tasks, including captioning, visual question answering,
                                detection, grounding, and OCR.
                                The OCR datasets were constructed using PaddleOCR to perform OCR on Chinese images from
                                Wukong and on English images from LaionCOCO, and were manually verified.
                                Besides, we also crawled and manually parsed the exam data from
                                <a href="https://uworld.com/" rel="nofollow">uworld</a>,
                                <a href="https://www.kaptest.com/act" rel="nofollow">kaptest</a>,
                                <a href="https://testbank.zip/" rel="nofollow">testbank</a>,
                                <a href="https://www.aqa.org.uk/" rel="nofollow">aga</a>,
                                and
                                <a href="https://satsuite.collegeboard.org/sat" rel="nofollow">sat</a>.
                                The interleaved data from
                                <a href="https://github.com/OpenGVLab/OmniCorpus" rel="nofollow">OmniCorpus</a>
                                was also utilized.</td></tr>
                          <tr><th>Trainable Module</th><td colspan="2">ViT + MLP</td><td colspan="1">MLP</td></tr>
                          <tr><th rowspan="2">Stage-2</th><th>Training Data</th><td colspan="3">We constructed the training data based on the 5M high-quality bilingual dataset used in
                            InternVL 1.5. Specifically, we included video data such as EgoTaskQA, Mementos, STAR,
                            NTU RGB+D, VideoChat2IT, and LSMDC-QA, as well as medical data such as
                            Medical-Diff-VQA, Pathology-VQA, PMC-CaseReport, PMC-VQA, Slake, and VQA-RAD.
                            We also included SROIE, FUNSD, and POIE to further enhance the model's ability to
                            recognize handwritten fonts.
                            Additionally, we excluded all data from ShareGPT-4V and replace it with data from
                            <a href="https://huggingface.co/datasets/OpenGVLab/ShareGPT-4o">ShareGPT-4o</a>.</td></tr>
                          <tr><th>Trainable Module</th><td colspan="3">ViT + MLP + LLM</td></tr>
                        </table>

                        
                    <h2 class="title">Domain Adaptation</h2>
                    <p>To further promote the adoption of our models, we develop a unified adaptation framework for Mini-InternVL, which enables our models to transfer and outperform specialized models in downstream tasks, including autonomous driving, medical images, and remote sensing. We hope to provide insights into the application of MLLMs.</p>

                    <p>In<a rel="nofollow" href="https://internvl.readthedocs.io/en/latest/internvl2.0/domain_adaptation.html"> Document</a>, we provide detailed information on the datasets and the fine-tuning process. </p>
                    <center>
                        <img class="responsive" width="80%" alt="image" src="images/framework.png"><br>
                    </center>
                    <h2 class="title">Citation</h2>
<pre><code>
    @article{gao2024mini,
        title={Mini-InternVL: A Flexible-Transfer Pocket Multimodal Model with 5\% Parameters and 90\% Performance},
        author={Gao, Zhangwei and Chen, Zhe and Cui, Erfei and Ren, Yiming and Wang, Weiyun and Zhu, Jinguo and Tian, Hao and Ye, Shenglong and He, Junjun and Zhu, Xizhou and others},
        journal={arXiv preprint arXiv:2410.16261},
        year={2024}
        }
    @article{chen2023internvl,
        title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks},
        author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and Li, Bin and Luo, Ping and Lu, Tong and Qiao, Yu and Dai, Jifeng},
        journal={arXiv preprint arXiv:2312.14238},
        year={2023}
    }
    @article{chen2024far,
        title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},
        author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
        journal={arXiv preprint arXiv:2404.16821},
        year={2024}
    }
  </code></pre>
                        <br><h4 class="title"><a href="../">🔙 Go Back</a></h4>
                    </div>

                </article>
            </div>
        </main>
        <footer class="site-footer h-card">
            <data class="u-url" href="/blog/"></data>
            <div class="wrapper">
                <p class="footer-colophon">
                </p>
            </div>
        </footer>
    </body>
</html>
