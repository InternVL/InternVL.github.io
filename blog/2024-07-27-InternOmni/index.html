<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <!-- Begin Jekyll SEO tag v2.8.0 -->
        <title>InternOmni:Extending InternVL with Audio Modality</title>
        <meta name="generator" content="Jekyll v3.9.4"/>
        <meta property="og:title" content="InternOmni"/>
        <meta name="author" content="cuierfei"/>
        <meta property="og:locale" content="en_US"/>
        <meta property="og:site_name" content="InternOmni"/>
        <meta property="og:type" content="article"/>
        <meta property="article:published_time" content="2024-01-30T12:33:38-06:00"/>
        <meta name="twitter:card" content="summary"/>
        <meta property="twitter:title" content="InternOmni"/>
        <!-- End Jekyll SEO tag -->
        <link rel="icon" href="https://github-production-user-asset-6210df.s3.amazonaws.com/47669167/330728723-7037290e-f474-4d11-b90f-1d8316087bf8.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240529%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240529T072300Z&X-Amz-Expires=300&X-Amz-Signature=d12b9e5c3c49a082747f5da55529a4f1247cd17b4329fafc1cb6d1c0678efa77&X-Amz-SignedHeaders=host&actor_id=23737120&key_id=0&repo_id=721995615">
        <link rel="stylesheet" href="/blog/assets/main.css">
        <link type="application/atom+xml" rel="alternate" href="https://llava-vl.github.io/blog/feed.xml" title="LLaVA"/>
    </head>
    <body>
        <header class="site-header" role="banner">
            <div class="wrapper">
                <a class="site-title" rel="author" href="/blog/">InternVL</a>
                <nav class="site-nav">
                    <input type="checkbox" id="nav-trigger" class="nav-trigger"/>
                    <label for="nav-trigger">
                        <span class="menu-icon">
                            <svg viewBox="0 0 18 15" width="18px" height="15px">
                                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
                            </svg>
                        </span>
                    </label>
                    <div class="trigger"></div>
                </nav>
            </div>
        </header>
        <main class="page-content" aria-label="Content">
            <style>
                @media (max-width: 768px) {
                        img.responsive {
                        width: 100% !important;
                    }
                }
            </style>
            <div class="wrapper">
                <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
                    <header class="post-header">
                        <h1 class="post-title p-name" itemprop="name headline">InternOmni: Extending InternVL with Audio Modality</h1>
                        <p class="post-meta">
                            <time class="dt-published" datetime="2024-01-30T12:33:38-06:00" itemprop="datePublished">2024/07/31
      </time>
                            ‚Ä¢
                            <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                                <span class="p-author h-card" itemprop="name">OpenGVLab
                                </span>
                            </span>
                        </p>
                    </header>

                    <p><a rel="nofollow" href="../">[üîô Go Back]</a><a rel="nofollow" href="https://github.com/OpenGVLab/InternVL">[üÜï Github]</a><a rel="nofollow" href="https://arxiv.org/abs/2312.14238">[üìú InternVL 1.0 Paper]</a>  <a rel="nofollow" href="https://arxiv.org/abs/2404.16821">[üìú InternVL 1.5 Report]</a>  <a rel="nofollow" href="https://internvl.opengvlab.com/">[üó®Ô∏è Chat Demo]</a>  <a rel="nofollow" href="https://huggingface.co/spaces/OpenGVLab/InternVL">[ü§ó HF Demo]</a>  <a rel="nofollow" href="https://modelscope.cn/models/OpenGVLab/Mini-InternVL-Chat-4B-V1-5/summary">[<img src="../2024-02-21-InternVL-1.2/images/modelscope_logo.png" width="20px" style="max-width: 100%;"> ModelScope]</a>  <a rel="nofollow" href="https://github.com/OpenGVLab/InternVL?tab=readme-ov-file#quick-start-with-huggingface">[üöÄ Quick Start]</a> <a rel="nofollow" href="https://internvl.readthedocs.io/en/latest/">[üìñ Document]</a></p>


                    <div class="post-content e-content" itemprop="articleBody">
                        <!-- ÊèíÂÖ•MarkdownËΩ¨Êç¢ÂêéÁöÑHTMLÂÜÖÂÆπ -->

                        <table>
                            <thead>
                                <tr>
                                    <th>Type</th>
                                    <th>Model</th>
                                    <th>Date</th>
                                    <th>Download</th>
                                    <th>Note</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Multimodal Large Language Models</td>
                                    <td>InternOmni</td>
                                    <td>2024.07.25</td>
                                    <td>ü§ó <a href="https://huggingface.co/OpenGVLab/InternOmni" rel="nofollow">HF link</a></td>
                                    <td>Extending InternVL's Modalities to Audio with Good Performance</td>
                                </tr>
                                <tr>
                                    <td>Vision Foundation Model</td>
                                    <td>InternViT-300M-448px</td>
                                    <td>2024.05.25</td>
                                    <td>ü§ó <a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px" rel="nofollow">HF link</a></td>
                                    <td>Distilled small vision foundation model with 300M parameters.</td>
                                </tr>
                                <tr>
                                    <td>Audio Foundation Model</td>
                                    <td>Whisper-large-v3</td>
                                    <td>2024.07.25</td>
                                    <td>ü§ó <a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px" rel="nofollow">HF link</a></td>
                                    <td>Pre-trained model for ASR and speech translation</td>
                                </tr>
                            </tbody>
                        </table>

                        <h2>InternOmni</h2>

                        <h3>Method</h3>

                        <p>We introduce InternOmni, an open-source multimodal large language model that adds audio input to the existing InternVL series. The goal is to enhance the modality of the InternVL series models and move further toward general artificial intelligence while providing users with a better experience. We employ the following designs:</p>
                        <ol>
                            <li><strong>Strong Vision Encoder:</strong> Building on the previously applied vision model InternViT-6B, we utilized distillation to create a lightweight vision foundation model, InternViT-300M. This enhances visual understanding while reducing the model size.</li>
                            <li><strong>Efficient Audio Encoder:</strong> We adopted OpenAI's open-source Whisper-large-v3 model, which has been trained on a large amount of audio data and has strong capabilities in speech recognition and translation.</li>
                            <li><strong>High-Quality Audio-Image Dataset:</strong> We carefully collected a high-quality audio-image dataset that covers common scenes and document images. This dataset is used for audio question-answering training on images, improving the model's performance in audio question-answering tasks.</li>
                        </ol>
                        <div style="text-align: center;">
                            <img src="./images/structure.png" alt="Model structure" style="width:100%; max-width:800px;">
                        </div>

                        <h3 id="model-card">Model Card</h3>

                        <table style="text-align: center;">
                          <tr><th colspan="2">Name</th><th>InternOmni</th></tr>

                          <tr><th colspan="2">Resolution</th><td>448 √ó 448</td></tr>
                            <tr><th rowspan="3"><nobr>Stage-1</nobr></th><th>Training Data</th><td>To ensure the proper alignment of audio data, we train on approximately 26 million data points, including datasets like GigaSpeech, CommonVoice, Libriheavy, and WENETSPEECH. The format used is: audio+text => text. At this stage, we freeze the ViT and its MLP, only keeping the audio-related components active.</td></tr>
                          <tr><th>Trainable Module</th><td>MLP_audio</td></tr>
                          <tr><th>Trainable Cost</th><td>64 GPUs, 4k steps, approximately 30 hours.</td></tr>
                            <tr><th rowspan="3">Stage-2</th><th>Training Data</th><td>We train on approximately 1.9 million open-source image-text instruction datasets, replacing the original text with audio. These datasets include TextVQA, GQA, OKVQA, ALLAVA, and others. The format used is audio+image => text. At this stage, we freeze both the ViT and Whisper encoders, only keeping the MLP layers used for alignment active </td></tr>
                          <tr><th>Trainable Module</th><td>MLP_audio</td></tr>
                          <tr><th>Trainable Cost</th><td>32 GPUs, 3k steps, approximately 15 hours.</td></tr>
                        </table>

                        <h3>Performance</h3>

                        <p>InternVL Omni did not use VL data for training in both the alignment and SFT stages; instead, it was trained entirely with audio data. However, it retained InternVL's powerful capabilities in handling complex image-text data, excelling in tasks such as scientific charts, general charts, documents, infographics, and OCR.</p>


                        <table>
                            <thead>
                                <tr>
                                    <th>name</th>
                                    <th>MMMU<br>(val)</br></th>
                                    <th>MathVista<br>(testmini)</br></th>
                                    <th>AI2D<br>(test)</br></th>
                                    <th>ChartQA<br>(test)</br></th>
                                    <th>DocVQA<br>(test)</br></th>
                                    <th>InfoVQA<br>(test)</br></th>
                                    <th>OCRBench</th>
                                    <th>MMB-EN<br>(test)</br></th>
                                    <th>MMB-CN<br>(test)</br></th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>GPT-4V*<br>(20240409)</td>
                                    <td>63.1 / 61.7</td>
                                    <td>58.1</td>
                                    <td>89.4</td>
                                    <td>78.1</td>
                                    <td>87.2</td>
                                    <td>-</td>
                                    <td>678</td>
                                    <td>81.0</td>
                                    <td>80.2</td>
                                </tr>
                                <tr>
                                    <td>Gemini Pro 1.5*</td>
                                    <td>58.5 / 60.6</td>
                                    <td>57.7</td>
                                    <td>80.3</td>
                                    <td>81.3</td>
                                    <td>86.5</td>
                                    <td>72.7</td>
                                    <td>754</td>
                                    <td>73.9</td>
                                    <td>73.8</td>
                                </tr>
                                <tr>
                                    <td>Claude3.5-Sonnet*</td>
                                    <td>68.3 / 65.9</td>
                                    <td>67.7</td>
                                    <td>94.7</td>
                                    <td>90.8</td>
                                    <td>95.2</td>
                                    <td>-</td>
                                    <td>788</td>
                                    <td>79.7</td>
                                    <td>80.7</td>
                                </tr>
                                <tr>
                                    <td>GPT-4o*<br>(20240513)</td>
                                    <td>69.1 / 69.2</td>
                                    <td>63.8</td>
                                    <td>94.2</td>
                                    <td>85.7</td>
                                    <td>92.8</td>
                                    <td>-</td>
                                    <td>736</td>
                                    <td>83.4</td>
                                    <td>82.1</td>
                                </tr>
                                <tr>
                                    <td>Cambrian-1</td>
                                    <td>49.7 / 50.4</td>
                                    <td>53.2</td>
                                    <td>79.7</td>
                                    <td>75.6</td>
                                    <td>75.5</td>
                                    <td>-</td>
                                    <td>600</td>
                                    <td>81.4</td>
                                    <td>-</td>
                                </tr>
    
                                <tr>
                                    <td>LLaVA-NeXT Qwen1.5</td>
                                    <td>50.1</td>
                                    <td>49.0</td>
                                    <td>80.4</td>
                                    <td>79.7</td>
                                    <td>85.7</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>80.5</td>
                                    <td>-</td>
                                </tr>
                                <tr style="background-color: rgb(255,248,227);">
                                    <td>
                                        <nobr>InternVL2-Pro</nobr>
                                    </td>
                                    <td>58.9 / 62.0</td>
                                    <td>66.3</td>
                                    <td>87.3 / 96.0</td>
                                    <td>87.1</td>
                                    <td>95.1</td>
                                    <td>83.3</td>
                                    <td>837</td>
                                    <td>87.8</td>
                                    <td>87.2</td>
                                </tr>
                                <tr>
                                    <td>InternVL2-8B</td>
                                    <td>49.3 / 51.2</td>
                                    <td>58.3</td>
                                    <td>83.8</td>
                                    <td>83.3</td>
                                    <td>91.6</td>
                                    <td>74.8</td>
                                    <td>794</td>
                                    <td>81.7</td>
                                    <td>81.2</td>
                                </tr>
                                <tr style="background-color: rgb(255,248,227);">
                                    <td>
                                        <nobr>InternOmni</nobr>
                                    </td>
                                    <td>49.3 / 51.2</td>
                                    <td>58.3</td>
                                    <td>83.8</td>
                                    <td>83.3</td>
                                    <td>91.6</td>
                                    <td>74.8</td>
                                    <td>794</td>
                                    <td>81.7</td>
                                    <td>81.2</td>
                                </tr>
                            </tbody>
                        </table>

                        <table>
                            <thead>
                                <tr>
                                    <th>name</th>
                                    <th>DocVQA_audio<br>(val)</th>
                                    <th>AI2D_audio<br>(test)</th>
                                    <th>Chartvqa_audio<br>(human)</th>
                                    <th>Chartvqa_audio<br>(augment)</th>
                                    <th>Textvqa_audio<br>(val)</th>
                                    <th>Infovqa_audio<br>(val)</th>

                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>InternOmni</td>
                                    <td>79.94</td>
                                    <td>53.92</td>
                                    <td>56.08</td>
                                    <td>76.48</td>
                                    <td>69.07</td>
                                    <td>60.34</td>
                                </tr>
                            </tbody>
                        </table>

                        <ul>
                            <li>We simultaneously use <a href="https://github.com/OpenGVLab/InternVL/tree/v2.0.0/internvl_chat/eval">InternVL</a> and <a href="https://github.com/open-compass/VLMEvalKit">VLMEvalKit</a> repositories for model evaluation. Specifically, the results reported for AI2D, ChartQA, DocVQA, InfoVQA, MMBench were tested using the InternVL repository. MathVista and OCRBench were evaluated using the VLMEvalKit.</li>
                            <li>For MMMU, we report both the original scores (left side: evaluated using the InternVL codebase for InternVL series models, and sourced from technical reports or webpages for other models) and the VLMEvalKit scores (right side: collected from the <a href="https://rank.opencompass.org.cn/leaderboard-multimodal/?m=REALTIME">OpenCompass leaderboard</a>).</li>
                          <li>Please note that evaluating the same model using different testing toolkits like InternVL and VLMEvalKit can result in slight differences, which is normal. Updates to code versions and variations in environment and hardware can also cause minor discrepancies in results.</li>
                        </ul>


                        <h3>Benchmark</h3>

                        <p>Existing audio benchmarks mainly focus on the audio itself, with relatively simple questions and images. To better evaluate the model's ability to handle complex audio-image pair problems, especially those involving tables and mathematical knowledge, I transcribed the text parts of some existing complex image-text VQA datasets and converted them into audio files. From these, I selected the more challenging data from the original image-text questions, creating a 15k audio-image question-answer dataset. The benchmark will be open-sourced soon.</p>
                        
                        <h3>Data</h3>
                        <table>
                            <thead>
                                <tr>
                                    <th>Type</th>
                                    <th>Size</th>
                                    <th>Download</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Pretrain Data</td>
                                    <td>26M</td>
                                    <td>ü§ó <a href="https://huggingface.co/datasets/OpenGVLab/ShareGPT-4o/tree/main/audio_pretrain" rel="nofollow">HF link</a></td>
                                </tr>
                                <tr>
                                    <td>STF Data</td>
                                    <td>1.9M</td>
                                    <td>TBD</td>
                                </tr>
                                <tr>
                                    <td>Benchmark</td>
                                    <td>15k</td>
                                    <td>ü§ó <a href="https://huggingface.co/datasets/OpenGVLab/ShareGPT-4o/tree/main/audio_benchmark" rel="nofollow">HF link</a></td>
                                </tr>
                            </tbody>
                        </table>


                        <h3>Examples</h3>
                        <div class="slider-container">
                            <div class="slider">
                                <div class="slider-inner">
                                    <div class="slide">
                                        <div class="slide-content">
                                            <h3>Input:</h3>
                                            <div class="input-box">
                                                <p style="text-align: center;">[Èü≥È¢ëÊñáÊú¨] Is it possible to make a left turn from this lane?</p>
                                                <div style="text-align: center;">
                                                    <audio controls>
                                                        <source src="./audios/00045.wav" type="audio/wav">
                                                        ÊÇ®ÁöÑÊµèËßàÂô®‰∏çÊîØÊåÅÈü≥È¢ëÂÖÉÁ¥†„ÄÇ
                                                    </audio>
                                                </div>
                                                <div style="text-align: center;">
                                                    <img src="./images/00045.jpg" alt="Input Image" style="max-width:300px; height:auto; margin-top:10px;">
                                                </div>
                                            </div>
                                            <h3>InternOmni:</h3>
                                            <div class="output-box">
                                                <p>No, the lane you're currently in seems to be a straight-only lane. However, there is a left turn signal at the next intersection, indicating that you can make a left turn from your current position.</p>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="slide">
                                        <div class="slide-content">
                                            <h3>Input:</h3>
                                            <div class="input-box">
                                                <p style="text-align: center;">[Èü≥È¢ëÊñáÊú¨] ÂÜô‰∏Ä‰∏™ÂØìË®ÄÊïÖ‰∫ã</p>
                                                <div style="text-align: center;">
                                                    <audio controls>
                                                        <source src="./audios/00082.mp3" type="audio/mp3">
                                                        ÊÇ®ÁöÑÊµèËßàÂô®‰∏çÊîØÊåÅÈü≥È¢ëÂÖÉÁ¥†„ÄÇ
                                                    </audio>
                                                </div>
                                                <div style="text-align: center;">
                                                    <img src="./images/00082.png" alt="Input Image" style="max-width:300px; height:auto; margin-top:10px;">
                                                </div>
                                            </div>
                                            <h3>InternOmni:</h3>
                                            <div class="output-box">
                                                <p>‰ªéÂâçÔºåÊúâ‰∏Ä‰∏™Âπ¥ËΩª‰∫∫Âè´Êùé‰∫ë„ÄÇ‰ªñÈùûÂ∏∏Âã§Â•ãÂ•ΩÂ≠¶Ôºå‰ΩÜÊòØÊÄªÊòØÊ≤°ÊúâÊú∫‰ºöÂ±ïÁ§∫Ëá™Â∑±ÁöÑÊâçÂçéÂíåËÉΩÂäõ„ÄÇÊúâ‰∏ÄÂ§©Ôºå‰ªñÂê¨ËØ¥ÈôÑËøëÁöÑÂ±±‰∏äÊúâ‰∏ÄÂè™Á•ûÂ•áÁöÑÈ∏°ÔºåÂè™Ë¶ÅËÉΩÂ§üÂæóÂà∞ÂÆÉÁöÑÁæΩÊØõÔºåÂ∞±ËÉΩËé∑ÂæóÊàêÂäüÂíåË¥¢ÂØå„ÄÇ‰∫éÊòØÔºå‰ªñÂÜ≥ÂÆöÂéªÂØªÊâæËøôÂè™Á•ûÂ•áÁöÑÈ∏°„ÄÇ\nÂú®Â±±ËÑö‰∏ãÔºåÊùé‰∫ëÈÅáÂà∞‰∫Ü‰∏ÄÂè™ÁúüÊ≠£ÁöÑÂÖ¨È∏°„ÄÇÂÆÉÁ´ôÂú®‰∏ÄÂùóÂ≤©Áü≥‰∏äÔºå‰ºº‰πéÊ≠£Âú®‰ºëÊÅØÊàñËÄÖËßÇÂØüÂë®Âõ¥ÁöÑÁéØÂ¢É„ÄÇÊùé‰∫ëËØïÂõæÊé•ËøëÈÇ£Âè™ÂÖ¨È∏°Ôºå‰ΩÜÊØèÊ¨°ÂΩì‰ªñÈù†ËøëÊó∂ÔºåÂÖ¨È∏°ÈÉΩ‰ºöÂèëÂá∫‰∏ÄÂ£∞Âìç‰∫ÆÁöÑÂïºÂè´Âπ∂È£ûËµ∞„ÄÇÂ∞ΩÁÆ°Â¶ÇÊ≠§ÔºåÊùé‰∫ëÂπ∂Ê≤°ÊúâÊîæÂºÉ‰ªñÁöÑÁõÆÊ†á„ÄÇ‰ªñ‰∏ÄÁõ¥Ë∑üÁùÄÂÖ¨È∏°Áà¨Â±±ÔºåÁõ¥Âà∞‰ªñ‰ª¨Âà∞ËææÂ±±È°∂„ÄÇÈÇ£ÈáåÊúâÂè¶‰∏ÄÂùóÂ§ßÁü≥Â§¥Âíå‰∏ÄÊ£µÊ†ë„ÄÇÂΩìÊùé‰∫ëËµ∞Âà∞ÈÇ£ÂùóÁü≥Â§¥ÁöÑÊóÅËæπÊó∂Ôºå‰ªñÁúãÂà∞‰∫Ü‰∏ÄÂè™Â∞èÈ∏üÂùêÂú®‰∏äÈù¢„ÄÇËøôÂè™È∏üÁúãËµ∑Êù•ÂÉèÊòØ‰∏ÄÂè™Èπ¶ÈπâÔºåÂõ†‰∏∫ÂÆÉÊúâÁùÄÈ≤úËâ≥ÁöÑÈªÑËâ≤„ÄÅÁªøËâ≤ÂíåËìùËâ≤ÁæΩÊØõ„ÄÇÁÑ∂ËÄåÔºåËøôÂπ∂‰∏çÊòØ‰º†ËØ¥‰∏≠ÁöÑÁ•ûÂ•á‰πãÈ∏ü‚Äî‚ÄîËÄåÊòØÂè¶‰∏ÄÂè™ÊôÆÈÄöÁöÑÈ∏üÁ±ª„ÄÇÂ§±Êúõ‰πã‰ΩôÔºåÊùé‰∫ëÊÑèËØÜÂà∞Ëá™Â∑±Â∑≤ÁªèËµ∞‰∫ÜÂæàËøúÁöÑË∑ØÁ®ãÔºå‰ΩÜ‰ªñ‰ªçÁÑ∂ÁªßÁª≠ÊîÄÁôªÁùÄËøôÂ∫ßÂ±±Â≥∞„ÄÇÊúÄÁªàÔºå‰ªñÂú®Â±±ÁöÑÂè¶‰∏ÄËæπÊâæÂà∞‰∫Ü‰∏ÄÂ∫ßÊÇ¨Â¥ñÂ≥≠Â£Å‰∏äÁöÑÊ¥ûÁ©¥„ÄÇÂú®ÈÇ£ÈáåÔºå‰ªñÂèëÁé∞‰∫Ü‰∏Ä‰∏™Á•ûÁßòÁöÑÂÆùÁÆ±„ÄÇËøô‰∏™ÁÆ±Â≠êË¢´ÈîÅ‰Ωè‰∫ÜÔºåÂπ∂‰∏îÈí•ÂåôÂ∞±Âú®ÈôÑËøëÁöÑÊ†ë‰∏äÊåÇÁùÄ„ÄÇ‰∏ÄÊó¶ÊâìÂºÄÁõíÂ≠êÔºåÈáåÈù¢Ë£ÖÊª°‰∫ÜÈáëÂ∏ÅÂíåÁè†ÂÆùÔºÅÊùé‰∫ëÊÑüÂà∞ÈùûÂ∏∏È´òÂÖ¥ÔºåÂõ†‰∏∫‰ªñÁªà‰∫éÂÆûÁé∞‰∫ÜËá™Â∑±ÁöÑÊ¢¶ÊÉ≥„ÄÇ‰ªñÊääÊâÄÊúâÁöÑÈí±ÈÉΩÂ∏¶ÂõûÂÆ∂ÔºåÁÑ∂ÂêéÂºÄÂßã‰∫ÜÊñ∞ÁöÑÁîüÊ¥ª„ÄÇ‰ªéÈÇ£Â§©Ëµ∑Ôºå‰ªñÂ∞±Êàê‰∏∫‰∫ÜÂΩìÂú∞ÊúÄÂØåÊúâÁöÑ‰∫∫‰πã‰∏ÄÔºåËÄå‰ªñ‰πüÊòéÁôΩÂà∞ÔºöÂè™Êúâ‰∏çÊñ≠Âä™ÂäõÔºåÊâçÊúâÂèØËÉΩËé∑ÂæóÊàêÂäü„ÄÇ</p>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="nav-buttons">
                                    <button class="nav-button" onclick="slideLeft()">&#10094;</button>
                                    <button class="nav-button" onclick="slideRight()">&#10095;</button>
                                </div>
                            </div>
                        </div>
                        
                        <script>
                            let currentIndex = 0;
                        
                            function slideLeft() {
                                if (currentIndex > 0) {
                                    currentIndex--;
                                    document.querySelector('.slider-inner').style.transform = `translateX(-${currentIndex * 100}%)`;
                                }
                            }
                        
                            function slideRight() {
                                if (currentIndex < document.querySelectorAll('.slide').length - 1) {
                                    currentIndex++;
                                    document.querySelector('.slider-inner').style.transform = `translateX(-${currentIndex * 100}%)`;
                                }
                            }
                        </script>
                        
                        <style>
                            .slider-container {
                                display: flex;
                                justify-content: center;
                                align-items: center;
                                margin-top: 20px;
                            }
                        
                            .slider {
                                width: 100%;
                                max-width: 800px;
                                overflow: hidden;
                                position: relative;
                                border: 1px solid #ddd;
                                border-radius: 8px;
                                box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.1);
                                background-color: #f9f9f9;
                            }
                        
                            .slider-inner {
                                display: flex;
                                transition: transform 0.5s ease;
                            }
                        
                            .slide {
                                min-width: 100%;
                                box-sizing: border-box;
                                padding: 20px;
                            }
                        
                            .slide-content {
                                max-width: 800px;
                                margin: 0 auto;
                            }
                        
                            .input-box, .output-box {
                                background-color: #ffffff;
                                border-radius: 8px;
                                padding: 15px;
                                box-shadow: 0px 2px 5px rgba(0, 0, 0, 0.1);
                                margin-bottom: 15px;
                            }
                        
                            .input-box p, .output-box p {
                                margin: 0;
                            }
                        
                            .nav-buttons {
                                position: absolute;
                                top: 50%;
                                transform: translateY(-50%);
                                width: 100%;
                                display: flex;
                                justify-content: space-between;
                            }
                        
                            .nav-button {
                                background: rgba(0, 0, 0, 0.5);
                                color: white;
                                border: none;
                                padding: 10px;
                                cursor: pointer;
                                border-radius: 50%;
                                outline: none;
                            }
                        
                            .nav-button:focus {
                                outline: none;
                            }
                        </style>
                        
                        



                        <h3 class="title">Citation</h3>
<pre><code>
  @article{chen2023internvl,
      title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks},
      author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and Li, Bin and Luo, Ping and Lu, Tong and Qiao, Yu and Dai, Jifeng},
      journal={arXiv preprint arXiv:2312.14238},
      year={2023}
  }
  @article{chen2024far,
    title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},
    author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
    journal={arXiv preprint arXiv:2404.16821},
    year={2024}
  }
  </code></pre>
                        <br><h4 class="title"><a href="../">üîô Go Back</a></h4>
</div>


                </article>
            </div>
        </main>
        <footer class="site-footer h-card">
        <!--   <data class="u-url" href="https://internvl.github.io/blog/">InternVL</data>--> 
            <p class="footer-colophon">
            </p>
        </footer>
    </body>
</html>
